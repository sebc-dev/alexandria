---
phase: 07-crawl-operations
plan: 04
type: execute
wave: 3
depends_on:
  - "07-01"
  - "07-02"
  - "07-03"
files_modified:
  - src/main/java/dev/alexandria/crawl/CrawlService.java
  - src/main/java/dev/alexandria/ingestion/IngestionService.java
  - src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
  - src/test/java/dev/alexandria/ingestion/IngestionServiceTest.java
autonomous: true
requirements:
  - CRWL-03
  - CRWL-06
  - CRWL-11

must_haves:
  truths:
    - "CrawlService applies scope filtering (allow/block patterns, max depth) during BFS crawl"
    - "CrawlService performs incremental crawl: skips pages with unchanged content hash, re-processes changed pages"
    - "CrawlService detects and cleans up deleted pages post-crawl"
    - "CrawlService handles llms-full.txt content via hybrid ingestion (ingest directly + fill gaps via crawl)"
    - "CrawlService tracks progress via CrawlProgressTracker during crawl"
  artifacts:
    - path: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      provides: "Evolved crawl orchestrator with scope, incremental, progress, llms.txt support"
      contains: "CrawlScope"
    - path: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      provides: "Ingestion with change detection and per-page deletion"
      contains: "ContentHasher"
    - path: "src/test/java/dev/alexandria/crawl/CrawlServiceTest.java"
      provides: "Updated unit tests for crawl orchestrator"
      min_lines: 80
  key_links:
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/crawl/UrlScopeFilter.java"
      via: "Scope filtering before crawling each URL"
      pattern: "UrlScopeFilter\\.isAllowed"
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java"
      via: "Progress updates during crawl loop"
      pattern: "progressTracker\\.record"
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      via: "Incremental ingest with hash-based change detection"
      pattern: "ingestionService\\.ingestPageIncremental"
    - from: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      to: "src/main/java/dev/alexandria/crawl/ContentHasher.java"
      via: "SHA-256 hash comparison for change detection"
      pattern: "ContentHasher\\.sha256"
---

<objective>
Evolve CrawlService and IngestionService to support scope-filtered crawling with depth tracking, incremental ingestion via content hash change detection, deleted page cleanup, llms-full.txt hybrid ingestion, and progress tracking.

Purpose: This is the core orchestration plan connecting all utilities (Plan 01-03) into a working crawl system that respects scope limits, skips unchanged pages, and tracks progress. Addresses the main functional requirements for crawl operations.
Output: Updated CrawlService with full scope/incremental/progress support. Updated IngestionService with hash-based change detection.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-crawl-operations/07-RESEARCH.md
@.planning/phases/07-crawl-operations/07-01-SUMMARY.md
@.planning/phases/07-crawl-operations/07-02-SUMMARY.md
@.planning/phases/07-crawl-operations/07-03-SUMMARY.md
@src/main/java/dev/alexandria/crawl/CrawlService.java
@src/main/java/dev/alexandria/ingestion/IngestionService.java
@src/main/java/dev/alexandria/ingestion/IngestionState.java
@src/main/java/dev/alexandria/ingestion/IngestionStateRepository.java
@src/main/java/dev/alexandria/crawl/CrawlScope.java
@src/main/java/dev/alexandria/crawl/UrlScopeFilter.java
@src/main/java/dev/alexandria/crawl/ContentHasher.java
@src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
@src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evolve CrawlService with scope filtering, depth tracking, progress, and llms-full.txt hybrid ingestion</name>
  <files>
    src/main/java/dev/alexandria/crawl/CrawlService.java
    src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
  </files>
  <action>
    **Evolve CrawlService** to accept CrawlScope and sourceId, applying scope filtering and depth tracking during BFS crawl, with progress reporting and llms-full.txt hybrid support.

    **New method signature** (replace or overload existing `crawlSite`):
    ```java
    public List<CrawlResult> crawlSite(UUID sourceId, String rootUrl, CrawlScope scope)
    ```
    - The old `crawlSite(String rootUrl, int maxPages)` should delegate to the new method, creating a default CrawlScope.

    **Add new constructor dependencies**:
    - `CrawlProgressTracker progressTracker` (from Plan 03)
    - `IngestionService ingestionService` (for incremental ingestion during crawl -- see Task 2)
    - `IngestionStateRepository ingestionStateRepository` (for change detection and orphan cleanup)
    - `EmbeddingStore<TextSegment> embeddingStore` (for per-page chunk deletion)

    **Depth tracking changes**:
    - Replace `LinkedHashSet<String>` queue with `LinkedHashMap<String, Integer>` mapping URL to its discovery depth (or use a `Deque<QueueEntry>` where `record QueueEntry(String url, int depth)`)
    - Seed URLs from discovery have depth 0
    - Discovered links get parent depth + 1
    - Before crawling a URL, check: if `scope.maxDepth() != null && depth > scope.maxDepth()`, skip (do not crawl, do not log as filtered)

    **Scope filtering changes**:
    - Before adding any URL to the queue (both seed and discovered links), check `UrlScopeFilter.isAllowed(url, rootUrl, scope)`
    - If URL is filtered, log it and call `progressTracker.recordFiltered(sourceId, url)` (per user decision: filtered URLs logged, available in crawl_status)
    - Apply scope filter to discovered links in `enqueueDiscoveredLinks`

    **Progress tracking changes**:
    - Call `progressTracker.startCrawl(sourceId, queue.size())` after seeding the queue
    - After each page crawl: `progressTracker.recordPageCrawled(sourceId)` (or `recordPageSkipped` if hash unchanged, or `recordError`)
    - When discovering new URLs that expand the queue: `progressTracker.updateTotal(sourceId, discoveredCount)`
    - After crawl completes: `progressTracker.completeCrawl(sourceId)` (or `failCrawl` on exception)

    **llms-full.txt hybrid ingestion**:
    - After page discovery, check if `DiscoveryResult.llmsFullContent()` is non-null
    - If present: ingest the llms-full.txt content directly via `ingestionService.ingestPage(llmsFullContent, rootUrl, Instant.now().toString())`
    - Track which URLs from the llms.txt list are "covered" by llms-full.txt (the URLs from `discovery.urls()` when method is LLMS_FULL_TXT). These are marked as already ingested.
    - During BFS crawl, skip pages whose URL is in the llms-full.txt covered set (they're already ingested)
    - This implements the hybrid approach: llms-full.txt provides the primary content, crawling fills any gaps

    **Deleted page cleanup** (post-crawl):
    - After the crawl loop completes, invoke `cleanupDeletedPages(sourceId, crawledUrls)`:
      1. `List<IngestionState> allStates = ingestionStateRepository.findAllBySourceId(sourceId)`
      2. Find orphaned URLs: states whose pageUrl is not in crawledUrls set
      3. For each orphaned URL: `embeddingStore.removeAll(metadataKey("source_url").isEqualTo(orphanUrl))`
      4. Delete the orphaned IngestionState records: `ingestionStateRepository.deleteAllBySourceIdAndPageUrlNotIn(sourceId, crawledUrls)`
    - Only run cleanup for incremental recrawls (not initial crawl where there are no prior states)

    **Updated CrawlServiceTest**:
    - Update existing tests for new constructor (mock new dependencies)
    - `scopeFilterRejectsBlockedUrls` -- verify URLs matching block patterns are not crawled
    - `scopeFilterAllowsMatchingUrls` -- verify only URLs matching allow patterns are crawled
    - `maxDepthLimitsUrlDiscovery` -- verify URLs beyond maxDepth are not enqueued
    - `progressTrackerUpdatedDuringCrawl` -- verify startCrawl, recordPageCrawled, completeCrawl called
    - `llmsFullContentIngestedDirectly` -- verify llms-full.txt content ingested, covered URLs skipped
    - Keep existing test behaviors (BFS, maxPages, link following, sitemap mode)
  </action>
  <verify>
    `./quality.sh test` passes. All CrawlService tests pass including new scope/depth/progress tests. Run `./gradlew test --tests "dev.alexandria.crawl.CrawlServiceTest"`.
  </verify>
  <done>CrawlService applies scope filtering (block > allow priority), respects max depth, tracks progress via CrawlProgressTracker, handles llms-full.txt hybrid ingestion, and cleans up deleted pages post-crawl.</done>
</task>

<task type="auto">
  <name>Task 2: IngestionService incremental ingestion with hash-based change detection</name>
  <files>
    src/main/java/dev/alexandria/ingestion/IngestionService.java
    src/test/java/dev/alexandria/ingestion/IngestionServiceTest.java
  </files>
  <action>
    **Add incremental ingestion method** to IngestionService:

    Add constructor dependencies:
    - `IngestionStateRepository ingestionStateRepository`
    - `EmbeddingStore<TextSegment> embeddingStore` (already available for chunk deletion -- currently only used transitively via storeChunks; need direct reference for removeAll)

    Wait -- IngestionService already has `embeddingStore` field. Good. Add `ingestionStateRepository`.

    **New method**: `public IngestResult ingestPageIncremental(UUID sourceId, String url, String markdown)`

    **IngestResult record** (nested or in `ingestion/` package):
    ```java
    public record IngestResult(int chunksStored, boolean skipped, boolean changed) {}
    ```
    - `skipped = true` means content hash unchanged, no processing done
    - `changed = true` means new or changed content, re-chunked and re-embedded

    **Implementation flow** (per research pattern):
    1. Normalize URL via `UrlNormalizer.normalize(url)`
    2. Compute hash: `String newHash = ContentHasher.sha256(markdown)`
    3. Look up existing state: `ingestionStateRepository.findBySourceIdAndPageUrl(sourceId, normalizedUrl)`
    4. **If existing hash matches**: return `IngestResult(0, true, false)` -- skip entirely
    5. **If hash differs or no record**:
       a. Delete old chunks: `embeddingStore.removeAll(metadataKey("source_url").isEqualTo(normalizedUrl))` (using `import static dev.langchain4j.store.embedding.filter.MetadataFilterBuilder.metadataKey`)
       b. Chunk and embed: call existing `ingestPage(markdown, normalizedUrl, Instant.now().toString())` which returns chunk count
       c. Update or create IngestionState:
          - If existing: `existing.setContentHash(newHash); existing.setLastIngestedAt(Instant.now()); ingestionStateRepository.save(existing)`
          - If new: `ingestionStateRepository.save(new IngestionState(sourceId, normalizedUrl, newHash))`
       d. Return `IngestResult(chunkCount, false, true)`

    **Keep existing methods** (`ingest(List<CrawlResult>)` and `ingestPage(...)`) unchanged for backward compatibility. The new `ingestPageIncremental` is used by the evolved CrawlService.

    **Updated IngestionServiceTest**:
    - Update existing tests for new constructor (mock ingestionStateRepository)
    - `ingestPageIncrementalSkipsUnchangedContent` -- same hash returns skipped=true, no embedding/store calls
    - `ingestPageIncrementalReprocessesChangedContent` -- different hash: deletes old chunks, re-chunks, re-embeds, updates state
    - `ingestPageIncrementalCreatesNewStateForNewPage` -- no existing state: creates new IngestionState
    - `ingestPageIncrementalDeletesOldChunksBeforeReingesting` -- verify removeAll called before addAll (ordering matters)
    - Keep all existing test behaviors
  </action>
  <verify>
    `./quality.sh test` passes. All IngestionService tests pass including new incremental tests. Run `./gradlew test --tests "dev.alexandria.ingestion.IngestionServiceTest"`.
  </verify>
  <done>IngestionService.ingestPageIncremental performs hash-based change detection: skips unchanged pages, deletes old chunks and re-ingests changed pages, and persists content hash state for future comparisons.</done>
</task>

</tasks>

<verification>
- `./quality.sh test` passes with all new and existing tests green
- CrawlService filters URLs by scope patterns during BFS crawl
- CrawlService respects maxDepth during link discovery
- CrawlService tracks progress and reports via CrawlProgressTracker
- IngestionService skips unchanged pages and re-ingests changed ones
- Deleted page cleanup removes orphaned chunks post-crawl
- llms-full.txt content ingested directly, gaps filled by crawling
</verification>

<success_criteria>
- CrawlService scope tests verify block > allow pattern priority
- CrawlService depth tests verify maxDepth enforcement
- IngestionService incremental tests verify skip/reprocess/new-page paths
- Progress tracker integration verified in CrawlService tests
- No regressions in existing ~130+ tests
</success_criteria>

<output>
After completion, create `.planning/phases/07-crawl-operations/07-04-SUMMARY.md`
</output>
