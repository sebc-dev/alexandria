---
phase: 07-crawl-operations
plan: 05
type: execute
wave: 4
depends_on:
  - "07-04"
files_modified:
  - src/main/java/dev/alexandria/mcp/McpToolService.java
  - src/test/java/dev/alexandria/mcp/McpToolServiceTest.java
autonomous: true
requirements:
  - CRWL-03
  - CRWL-07
  - CRWL-09
  - CRWL-10

must_haves:
  truths:
    - "add_source MCP tool accepts scope parameters and triggers crawl orchestration"
    - "crawl_status MCP tool returns real-time progress for active crawls and summary for completed crawls"
    - "recrawl_source MCP tool triggers incremental recrawl with optional scope override and full flag"
    - "No automatic/scheduled recrawls -- only manual recrawl_source (CRWL-07 satisfied as deferred)"
  artifacts:
    - path: "src/main/java/dev/alexandria/mcp/McpToolService.java"
      provides: "Real MCP tool implementations replacing stubs"
      contains: "crawlService.crawlSite"
    - path: "src/test/java/dev/alexandria/mcp/McpToolServiceTest.java"
      provides: "Updated unit tests for all 6 MCP tools"
      min_lines: 100
  key_links:
    - from: "src/main/java/dev/alexandria/mcp/McpToolService.java"
      to: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      via: "addSource and recrawlSource invoke CrawlService.crawlSite"
      pattern: "crawlService\\.crawlSite"
    - from: "src/main/java/dev/alexandria/mcp/McpToolService.java"
      to: "src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java"
      via: "crawlStatus reads from progress tracker"
      pattern: "progressTracker\\.getProgress"
    - from: "src/main/java/dev/alexandria/mcp/McpToolService.java"
      to: "src/main/java/dev/alexandria/source/Source.java"
      via: "addSource creates Source with scope config, recrawl reads scope from Source"
      pattern: "source\\.toCrawlScope"
---

<objective>
Replace the stub MCP tool implementations in McpToolService with real orchestration: add_source triggers crawl with scope parameters, crawl_status returns real-time progress, recrawl_source triggers incremental recrawl.

Purpose: This is the user-facing layer that connects MCP tools to the crawl orchestration built in Plans 01-04. After this plan, users can add sources with scope control, monitor crawl progress, and trigger recrawls -- all through Claude Code MCP tools.
Output: Fully functional add_source, crawl_status, and recrawl_source MCP tools.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-crawl-operations/07-RESEARCH.md
@.planning/phases/07-crawl-operations/07-04-SUMMARY.md
@src/main/java/dev/alexandria/mcp/McpToolService.java
@src/main/java/dev/alexandria/crawl/CrawlService.java
@src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
@src/main/java/dev/alexandria/crawl/CrawlScope.java
@src/main/java/dev/alexandria/source/Source.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace MCP tool stubs with real crawl orchestration</name>
  <files>
    src/main/java/dev/alexandria/mcp/McpToolService.java
  </files>
  <action>
    **Add new constructor dependencies** to McpToolService:
    - `CrawlService crawlService`
    - `CrawlProgressTracker progressTracker`
    - `IngestionService ingestionService` (if needed for chunk count updates)

    **Replace addSource** tool:
    - Add optional scope parameters to `@ToolParam`:
      ```java
      @Tool(name = "add_source",
            description = "Add a documentation source URL for crawling and indexing. "
                        + "Optionally specify scope controls: allow/block URL patterns (glob), max depth, max pages.")
      public String addSource(
          @ToolParam(description = "URL of the documentation site to index") String url,
          @ToolParam(description = "Human-readable name for this source") String name,
          @ToolParam(description = "Comma-separated glob patterns for allowed URL paths (e.g., '/docs/**,/api/**'). Empty = allow all.", required = false) String allowPatterns,
          @ToolParam(description = "Comma-separated glob patterns for blocked URL paths (e.g., '/archive/**,/old/**'). Empty = block none.", required = false) String blockPatterns,
          @ToolParam(description = "Maximum crawl depth from root URL. Null = unlimited.", required = false) Integer maxDepth,
          @ToolParam(description = "Maximum number of pages to crawl (default: 500).", required = false) Integer maxPages,
          @ToolParam(description = "Manual llms.txt URL if auto-detection fails.", required = false) String llmsTxtUrl)
      ```
    - Create Source with scope fields populated:
      ```java
      Source source = new Source(url, name);
      source.setAllowPatterns(allowPatterns);
      source.setBlockPatterns(blockPatterns);
      source.setMaxDepth(maxDepth);
      source.setMaxPages(maxPages != null ? maxPages : 500);
      source.setLlmsTxtUrl(llmsTxtUrl);
      source.setStatus(SourceStatus.CRAWLING);
      sourceRepository.save(source);
      ```
    - Trigger async crawl (use virtual threads -- `Thread.startVirtualThread(() -> ...)` or `CompletableFuture.runAsync()`):
      ```java
      UUID sourceId = source.getId();
      CrawlScope scope = source.toCrawlScope();
      Thread.startVirtualThread(() -> {
          try {
              List<CrawlResult> results = crawlService.crawlSite(sourceId, url, scope);
              // IMPORTANT: Re-fetch the source entity inside the virtual thread to avoid
              // JPA session boundary errors. The outer `source` reference is detached
              // from the JPA session once the original request thread completes.
              Source freshSource = sourceRepository.findById(sourceId).orElseThrow();
              freshSource.setStatus(SourceStatus.INDEXED);
              freshSource.setLastCrawledAt(Instant.now());
              freshSource.setChunkCount(/* count from results */);
              sourceRepository.save(freshSource);
          } catch (Exception e) {
              Source freshSource = sourceRepository.findById(sourceId).orElseThrow();
              freshSource.setStatus(SourceStatus.ERROR);
              sourceRepository.save(freshSource);
          }
      });
      ```
      **JPA session boundary rule**: Never use a JPA entity reference captured from the outer scope inside a virtual thread. Always re-fetch by ID within the new thread. This applies to both the addSource and recrawlSource async blocks.
    - Return immediately with source ID and status message: `"Source '%s' created (ID: %s). Crawl started. Check progress with crawl_status.".formatted(name, source.getId())`

    **Replace crawlStatus** tool:
    - Check CrawlProgressTracker first (for active crawls):
      ```java
      Optional<CrawlProgress> progress = progressTracker.getProgress(uuid);
      if (progress.isPresent()) {
          CrawlProgress p = progress.get();
          // Format real-time progress
          return formatActiveProgress(source, p);
      }
      ```
    - If no active crawl, show summary from Source entity (for completed/idle sources):
      ```java
      return formatCompletedSummary(source);
      ```
    - `formatActiveProgress`: structured text with source name, URL, status, pages crawled/total, skipped, errors (with URLs), filtered (with URLs), elapsed time. Follow format from 07-RESEARCH.md discretion recommendation.
    - `formatCompletedSummary`: source name, URL, status, last crawl time, chunk count.
    - Include filtered URLs in active progress output (per user decision: "URLs filtered by scope are logged, available in crawl_status output")

    **Replace recrawlSource** tool:
    - Add scope override and full flag parameters:
      ```java
      @Tool(name = "recrawl_source",
            description = "Trigger a recrawl of an existing documentation source. "
                        + "Incremental by default (only re-processes changed pages). Use full=true to force complete re-processing.")
      public String recrawlSource(
          @ToolParam(description = "UUID of the source to re-crawl") String sourceId,
          @ToolParam(description = "Force full recrawl (re-process all pages, not just changed ones)", required = false) Boolean full,
          @ToolParam(description = "Override allow patterns for this crawl run (comma-separated globs)", required = false) String allowPatterns,
          @ToolParam(description = "Override block patterns for this crawl run (comma-separated globs)", required = false) String blockPatterns,
          @ToolParam(description = "Override max depth for this crawl run", required = false) Integer maxDepth,
          @ToolParam(description = "Override max pages for this crawl run", required = false) Integer maxPages)
      ```
    - Look up source, validate it exists and is not currently CRAWLING
    - Build CrawlScope: use override params if provided, else fall back to source's stored scope config (per user decision: "overrides don't persist, they're one-time for that crawl run")
    - If `full == true`: clear all IngestionState records for this source (`ingestionStateRepository.deleteAllBySourceId(sourceId)`) so every page is re-processed
    - Update source status to UPDATING
    - Trigger async crawl via virtual thread (same pattern as addSource)
    - Return: `"Recrawl started for '%s' (%s mode). Check progress with crawl_status.".formatted(source.getName(), full ? "full" : "incremental")`

    **Important**: Scope overrides on recrawl do NOT persist to the Source entity. They are used only for constructing the CrawlScope for this crawl run. The Source entity's scope fields remain unchanged.

    **Update Javadoc** on McpToolService class to reflect the tools are now fully functional (remove "stub" references).
  </action>
  <verify>
    `./quality.sh test` passes. All McpToolService tests pass including new ones. Verify async crawl does not block the MCP response.
  </verify>
  <done>add_source creates source with scope config and triggers async crawl. crawl_status returns real-time progress during crawl and summary after. recrawl_source triggers incremental or full recrawl with optional scope overrides.</done>
</task>

<task type="auto">
  <name>Task 2: Update McpToolService unit tests for real implementations</name>
  <files>
    src/test/java/dev/alexandria/mcp/McpToolServiceTest.java
  </files>
  <action>
    **Update existing tests** for new constructor signature (mock new dependencies: CrawlService, CrawlProgressTracker).

    **Update addSource tests**:
    - `addSourceCreatesSourceWithScopeAndTriggersCrawl` -- verify Source saved with scope fields, verify crawl triggered asynchronously
    - `addSourceWithNullScopeUsesDefaults` -- verify defaults (empty patterns, null maxDepth, 500 maxPages)
    - `addSourceReturnsImmediatelyWithId` -- verify response includes source ID and "Crawl started" message
    - Keep existing `addSourceWithEmptyUrlReturnsError` test

    **Update crawlStatus tests**:
    - `crawlStatusShowsActiveProgress` -- mock progressTracker returning CrawlProgress with CRAWLING status, verify output includes pages crawled/total/errors
    - `crawlStatusShowsCompletedSummary` -- mock progressTracker returning empty (no active crawl), verify output shows source entity summary
    - `crawlStatusShowsFilteredUrls` -- mock progress with filtered URLs, verify they appear in output
    - Keep existing `crawlStatusWithInvalidIdReturnsError` and `crawlStatusWithMissingSourceReturnsError` tests

    **Update recrawlSource tests**:
    - `recrawlSourceTriggersIncrementalByDefault` -- verify crawlService.crawlSite called, IngestionState NOT cleared
    - `recrawlSourceWithFullFlagClearsStateFirst` -- verify ingestionStateRepository.deleteAllBySourceId called when full=true
    - `recrawlSourceWithScopeOverridesUsesOverrides` -- verify CrawlScope built from override params, not source entity fields
    - `recrawlSourceRejectsActivelyRunningCrawl` -- source with CRAWLING status returns error message
    - Keep existing `recrawlSourceWithInvalidIdReturnsError` and `recrawlSourceWithMissingSourceReturnsError` tests

    **Testing async behavior**:
    - For async crawl tests, verify the crawlService is invoked but don't wait for completion. Use `verify(crawlService, timeout(1000)).crawlSite(...)` or simply verify that the method returns immediately with the expected message without blocking.
    - Alternatively, extract the async dispatch into a package-private method that tests can intercept, or use `@VisibleForTesting` to make the crawl runnable testable.

    **Follow existing test patterns**: Use `@ExtendWith(MockitoExtension.class)` with `@Mock` and `@InjectMocks`. Follow BDDMockito `given/willReturn` style per Phase 5 patterns.
  </action>
  <verify>
    `./quality.sh test` passes. All McpToolService tests pass. Run `./gradlew test --tests "dev.alexandria.mcp.McpToolServiceTest"`. All ~25+ tests green.
  </verify>
  <done>All 6 MCP tool methods have comprehensive unit tests. add_source, crawl_status, and recrawl_source tests verify real orchestration behavior. No regressions in existing search_docs and list_sources tests.</done>
</task>

</tasks>

<verification>
- `./quality.sh test` passes with all tests green
- add_source creates source with scope config and returns immediately
- crawl_status shows real-time progress for active crawls, summary for completed
- recrawl_source supports full and incremental modes with scope overrides
- No automatic scheduling -- only manual recrawl_source (CRWL-07 satisfied)
- Filtered URLs appear in crawl_status output (per user decision)
- Scope overrides on recrawl are one-time, not persisted to Source
</verification>

<success_criteria>
- All MCP tool tests verify real behavior (not stub responses)
- add_source with scope params verified in tests
- crawl_status active/completed/filtered outputs verified
- recrawl_source incremental/full/override modes verified
- Async crawl does not block MCP response
- No regressions in existing ~130+ tests
</success_criteria>

<output>
After completion, create `.planning/phases/07-crawl-operations/07-05-SUMMARY.md`
</output>
