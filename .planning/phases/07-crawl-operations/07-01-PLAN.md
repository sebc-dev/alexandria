---
phase: 07-crawl-operations
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/main/java/dev/alexandria/crawl/CrawlScope.java
  - src/main/java/dev/alexandria/crawl/UrlScopeFilter.java
  - src/main/java/dev/alexandria/crawl/ContentHasher.java
  - src/test/java/dev/alexandria/crawl/CrawlScopeTest.java
  - src/test/java/dev/alexandria/crawl/UrlScopeFilterTest.java
  - src/test/java/dev/alexandria/crawl/ContentHasherTest.java
autonomous: true
requirements:
  - CRWL-03
  - CRWL-06

must_haves:
  truths:
    - "CrawlScope record holds immutable scope configuration (allow patterns, block patterns, max depth, max pages)"
    - "UrlScopeFilter correctly filters URLs by glob patterns on URL paths, with block patterns taking priority"
    - "ContentHasher produces deterministic SHA-256 hex strings from content"
  artifacts:
    - path: "src/main/java/dev/alexandria/crawl/CrawlScope.java"
      provides: "Immutable scope config record"
      contains: "record CrawlScope"
    - path: "src/main/java/dev/alexandria/crawl/UrlScopeFilter.java"
      provides: "Static URL scope filtering utility"
      contains: "isAllowed"
    - path: "src/main/java/dev/alexandria/crawl/ContentHasher.java"
      provides: "SHA-256 content hashing utility"
      contains: "sha256"
    - path: "src/test/java/dev/alexandria/crawl/UrlScopeFilterTest.java"
      provides: "Unit tests for URL scope filtering"
      min_lines: 40
    - path: "src/test/java/dev/alexandria/crawl/ContentHasherTest.java"
      provides: "Unit tests for content hashing"
      min_lines: 15
  key_links:
    - from: "src/main/java/dev/alexandria/crawl/UrlScopeFilter.java"
      to: "src/main/java/dev/alexandria/crawl/CrawlScope.java"
      via: "UrlScopeFilter.isAllowed takes CrawlScope parameter"
      pattern: "isAllowed.*CrawlScope"
    - from: "src/main/java/dev/alexandria/crawl/UrlScopeFilter.java"
      to: "src/main/java/dev/alexandria/crawl/UrlNormalizer.java"
      via: "isSameSite check before scope pattern matching"
      pattern: "UrlNormalizer\\.isSameSite"
---

<objective>
TDD-develop three pure utility classes for crawl scope control and content hashing: CrawlScope record, UrlScopeFilter static utility, and ContentHasher static utility.

Purpose: These are the foundational building blocks for scope-limited crawling (CRWL-03) and incremental change detection (CRWL-06). All are pure functions with defined I/O, ideal for TDD.
Output: Three production classes with comprehensive test suites.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/main/java/dev/alexandria/crawl/UrlNormalizer.java
@src/main/java/dev/alexandria/crawl/CrawlResult.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD CrawlScope record and UrlScopeFilter utility</name>
  <files>
    src/main/java/dev/alexandria/crawl/CrawlScope.java
    src/main/java/dev/alexandria/crawl/UrlScopeFilter.java
    src/test/java/dev/alexandria/crawl/CrawlScopeTest.java
    src/test/java/dev/alexandria/crawl/UrlScopeFilterTest.java
  </files>
  <action>
    **RED phase: Write tests first.**

    **CrawlScope record** (`crawl/CrawlScope.java`):
    - Fields: `List<String> allowPatterns`, `List<String> blockPatterns`, `Integer maxDepth` (null = unlimited), `int maxPages`
    - Compact constructor: null-safe `List.copyOf()` for both pattern lists (follow existing record pattern from CrawlResult)
    - Default factory: `CrawlScope.withDefaults(int maxPages)` returning empty allow/block, null maxDepth

    Tests for CrawlScope (`CrawlScopeTest`):
    - `nullPatternsDefaultToEmptyLists` -- verify null allow/block become empty
    - `patternsAreDefensivelyCopied` -- verify List.copyOf immutability
    - `withDefaultsReturnsEmptyScope` -- verify factory method

    **UrlScopeFilter** (`crawl/UrlScopeFilter.java`):
    - Static utility class (private constructor), similar to UrlNormalizer pattern
    - `public static boolean isAllowed(String url, String rootUrl, CrawlScope scope)`:
      1. Check `UrlNormalizer.isSameSite(rootUrl, url)` first -- reject external URLs
      2. Extract URL path using `URI.create(url).getPath()`
      3. Check block patterns first: if any matches, return false (block takes priority per user decision)
      4. If allowPatterns is non-empty, URL must match at least one allow pattern
      5. If allowPatterns is empty, all same-site URLs are allowed
    - Use `java.nio.file.FileSystems.getDefault().getPathMatcher("glob:" + pattern)` on `Path.of(urlPath)`
    - Block pattern convention: strip `!` prefix before matching (user provides `!/docs/archive/**`, store as `/docs/archive/**` in blockPatterns list, or handle `!` at match time -- recommend stripping at parse time in CrawlScope, so blockPatterns stores clean patterns without `!`)

    Tests for UrlScopeFilter (`UrlScopeFilterTest`):
    - `externalUrlIsRejected` -- different host returns false
    - `sameSiteUrlWithNoScopeIsAllowed` -- empty allow/block = allow all
    - `urlMatchingAllowPatternIsAllowed` -- `/docs/api/v2` matches `/docs/**`
    - `urlNotMatchingAllowPatternIsRejected` -- `/blog/post` does not match `/docs/**`
    - `blockPatternTakesPriorityOverAllow` -- `/docs/archive/old` blocked by `/docs/archive/**` even though `/docs/**` allows it
    - `rootPathSlashIsAllowed` -- edge case: path `/` with no patterns
    - `multipleAllowPatternsAnyMatch` -- URL matching any of multiple allow patterns passes
    - `malformedUrlReturnsFalse` -- graceful handling of bad input

    **GREEN phase: Implement to pass tests.**

    **REFACTOR: Clean up if needed.**
  </action>
  <verify>
    `./quality.sh test` passes. All UrlScopeFilter and CrawlScope tests pass. Run `./gradlew test --tests "dev.alexandria.crawl.UrlScopeFilterTest" --tests "dev.alexandria.crawl.CrawlScopeTest"` to verify specifically.
  </verify>
  <done>CrawlScope record stores immutable scope config with null-safe defensive copies. UrlScopeFilter correctly filters URLs by glob patterns, with block patterns taking priority over allow patterns, and external URLs always rejected.</done>
</task>

<task type="auto">
  <name>Task 2: TDD ContentHasher utility</name>
  <files>
    src/main/java/dev/alexandria/crawl/ContentHasher.java
    src/test/java/dev/alexandria/crawl/ContentHasherTest.java
  </files>
  <action>
    **RED phase: Write tests first.**

    **ContentHasher** (`crawl/ContentHasher.java`):
    - Static utility class (private constructor), same pattern as UrlNormalizer
    - `public static String sha256(String content)`: returns lowercase hex string of SHA-256 hash
    - Uses `java.security.MessageDigest.getInstance("SHA-256")` and `java.util.HexFormat.of().formatHex(hash)`
    - `StandardCharsets.UTF_8` for encoding
    - Wraps `NoSuchAlgorithmException` in `IllegalStateException` (SHA-256 always available)

    Tests for ContentHasher (`ContentHasherTest`):
    - `hashOfKnownInputMatchesExpected` -- verify against a known SHA-256 value (e.g., SHA-256 of "hello" = "2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824")
    - `sameContentProducesSameHash` -- deterministic
    - `differentContentProducesDifferentHash`
    - `emptyStringProducesValidHash` -- edge case: empty string has a known SHA-256

    **GREEN phase: Implement to pass tests.**

    **REFACTOR: Clean up if needed.**
  </action>
  <verify>
    `./gradlew test --tests "dev.alexandria.crawl.ContentHasherTest"` passes. All 4 tests green.
  </verify>
  <done>ContentHasher.sha256() produces deterministic SHA-256 hex strings from content, verified against known test vectors.</done>
</task>

</tasks>

<verification>
- `./quality.sh test` passes with all new tests green
- CrawlScope stores scope config with immutable pattern lists
- UrlScopeFilter.isAllowed correctly applies glob patterns on URL paths
- ContentHasher.sha256 produces correct SHA-256 hex hashes
- All classes follow existing static utility patterns (private constructor, Javadoc)
</verification>

<success_criteria>
- All ~12 new unit tests pass
- UrlScopeFilter handles all edge cases: external URLs, empty scope, block priority, malformed URLs
- ContentHasher matches known SHA-256 test vectors
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/07-crawl-operations/07-01-SUMMARY.md`
</output>
