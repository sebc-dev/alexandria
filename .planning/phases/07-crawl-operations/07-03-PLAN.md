---
phase: 07-crawl-operations
plan: 03
type: execute
wave: 2
depends_on:
  - "07-01"
  - "07-02"
files_modified:
  - src/main/resources/db/migration/V2__source_scope_columns.sql
  - src/main/java/dev/alexandria/source/Source.java
  - src/main/java/dev/alexandria/ingestion/IngestionStateRepository.java
  - src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
  - src/main/java/dev/alexandria/crawl/CrawlProgress.java
  - src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
  - src/test/java/dev/alexandria/crawl/CrawlProgressTrackerTest.java
  - src/test/java/dev/alexandria/crawl/PageDiscoveryServiceTest.java
  - src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
autonomous: true
requirements:
  - CRWL-03
  - CRWL-06
  - CRWL-09
  - CRWL-11

must_haves:
  truths:
    - "Source entity stores scope config (allow patterns, block patterns, max depth, max pages) persisted via Flyway migration"
    - "IngestionStateRepository supports querying all states for a source and deleting orphaned pages"
    - "CrawlProgressTracker provides thread-safe in-memory progress tracking for active crawls"
    - "PageDiscoveryService tries llms.txt first, then sitemap.xml, then link crawl"
  artifacts:
    - path: "src/main/resources/db/migration/V2__source_scope_columns.sql"
      provides: "Schema migration for scope columns on sources table"
      contains: "ALTER TABLE sources"
    - path: "src/main/java/dev/alexandria/source/Source.java"
      provides: "Source entity with scope config fields"
      contains: "allowPatterns"
    - path: "src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java"
      provides: "Thread-safe in-memory crawl progress tracking"
      contains: "ConcurrentHashMap"
    - path: "src/main/java/dev/alexandria/crawl/CrawlProgress.java"
      provides: "Immutable crawl progress snapshot record"
      contains: "record CrawlProgress"
    - path: "src/main/java/dev/alexandria/ingestion/IngestionStateRepository.java"
      provides: "Extended repository with source-level query methods for incremental crawling"
      contains: "findAllBySourceId"
    - path: "src/main/java/dev/alexandria/crawl/PageDiscoveryService.java"
      provides: "Updated discovery with llms.txt priority cascade"
      contains: "llmsTxt"
  key_links:
    - from: "src/main/java/dev/alexandria/source/Source.java"
      to: "src/main/resources/db/migration/V2__source_scope_columns.sql"
      via: "JPA entity fields matching Flyway columns"
      pattern: "allow_patterns|block_patterns|max_depth|max_pages"
    - from: "src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java"
      to: "src/main/java/dev/alexandria/crawl/CrawlProgress.java"
      via: "Tracker stores/updates CrawlProgress records by source ID"
      pattern: "ConcurrentHashMap.*CrawlProgress"
    - from: "src/main/java/dev/alexandria/crawl/PageDiscoveryService.java"
      to: "src/main/java/dev/alexandria/crawl/LlmsTxtParser.java"
      via: "PageDiscoveryService calls LlmsTxtParser.parseUrls for URL extraction"
      pattern: "LlmsTxtParser"
---

<objective>
Build the infrastructure layer for Phase 7: database schema extension, Source entity scope fields, IngestionStateRepository query extensions, CrawlProgressTracker for real-time status, and PageDiscoveryService llms.txt cascade.

Purpose: Provides the data layer and tracking infrastructure that the crawl orchestrator (Plan 04) and MCP tools (Plan 05) will build upon. Schema changes must be in place before any code uses the new columns.
Output: V2 migration, updated Source entity, extended repository, progress tracker, and discovery cascade.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-crawl-operations/07-RESEARCH.md
@src/main/resources/db/migration/V1__initial_schema.sql
@src/main/java/dev/alexandria/source/Source.java
@src/main/java/dev/alexandria/ingestion/IngestionStateRepository.java
@src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
@src/main/java/dev/alexandria/crawl/SitemapParser.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: V2 Flyway migration, Source entity scope fields, and IngestionStateRepository extensions</name>
  <files>
    src/main/resources/db/migration/V2__source_scope_columns.sql
    src/main/java/dev/alexandria/source/Source.java
    src/main/java/dev/alexandria/ingestion/IngestionStateRepository.java
  </files>
  <action>
    **V2 Flyway migration** (`V2__source_scope_columns.sql`):
    Add scope configuration columns to sources table:
    ```sql
    ALTER TABLE sources ADD COLUMN allow_patterns TEXT[];
    ALTER TABLE sources ADD COLUMN block_patterns TEXT[];
    ALTER TABLE sources ADD COLUMN max_depth INTEGER;
    ALTER TABLE sources ADD COLUMN max_pages INTEGER DEFAULT 500;
    ALTER TABLE sources ADD COLUMN llms_txt_url TEXT;
    ```
    Use PostgreSQL native TEXT[] array type for pattern lists (JPA maps these via `@Column(columnDefinition = "text[]")` with `String[]` or use a converter). Default max_pages to 500 (reasonable safety limit). max_depth NULL = unlimited (per user decision). llms_txt_url allows manual llms.txt URL override.

    **Source entity updates** (`source/Source.java`):
    Add new fields with JPA column mappings:
    - `String[] allowPatterns` with `@Column(name = "allow_patterns", columnDefinition = "text[]")` -- NOTE: Hibernate does not natively map text[], so use `@JdbcTypeCode(SqlTypes.ARRAY)` from `org.hibernate.annotations.JdbcTypeCode` and `org.hibernate.type.SqlTypes`. Alternatively, store as comma-separated TEXT and parse in Java. **Recommendation**: use simple TEXT columns (`allow_patterns_csv TEXT`, `block_patterns_csv TEXT`) storing comma-separated glob patterns. This avoids Hibernate array type complexity. Update migration accordingly if choosing this approach.
    - Actually, given the complexity of PostgreSQL array types with Hibernate and this project's pragmatic architecture, use **JSONB column** for scope config: `scope_config JSONB` storing `{"allow": [...], "block": [...], "maxDepth": N, "maxPages": N, "llmsTxtUrl": "..."}`. But this also adds complexity.
    - **Simplest approach per project conventions**: individual TEXT columns for each, with patterns stored as JSON arrays (`TEXT` column containing `["\/docs\/**", "\/api\/**"]`). Use Jackson ObjectMapper to serialize/deserialize in getter/setter. Actually the simplest: just use individual columns with patterns as comma-separated strings.
    - **FINAL DECISION**: Use individual nullable columns. `allow_patterns TEXT` (comma-separated glob patterns), `block_patterns TEXT` (comma-separated glob patterns), `max_depth INTEGER`, `max_pages INTEGER DEFAULT 500`, `llms_txt_url TEXT`. Parse comma-separated patterns in a helper method `getBlockPatternList()` / `getAllowPatternList()` that returns `List<String>`. This is the simplest JPA mapping and avoids any Hibernate type adapter complexity.

    Update migration SQL:
    ```sql
    ALTER TABLE sources ADD COLUMN allow_patterns TEXT;
    ALTER TABLE sources ADD COLUMN block_patterns TEXT;
    ALTER TABLE sources ADD COLUMN max_depth INTEGER;
    ALTER TABLE sources ADD COLUMN max_pages INTEGER DEFAULT 500;
    ALTER TABLE sources ADD COLUMN llms_txt_url TEXT;
    ```

    Add to Source entity:
    - `private String allowPatterns;` with `@Column(name = "allow_patterns")`
    - `private String blockPatterns;` with `@Column(name = "block_patterns")`
    - `private Integer maxDepth;` with `@Column(name = "max_depth")`
    - `private Integer maxPages;` with `@Column(name = "max_pages")`
    - `private String llmsTxtUrl;` with `@Column(name = "llms_txt_url")`
    - Helper methods: `getAllowPatternList()` returns `List<String>` by splitting on `,` (trimmed), empty string = empty list
    - Helper methods: `getBlockPatternList()` returns `List<String>` same pattern
    - `toCrawlScope()` method that builds a `CrawlScope` record from the entity fields (convenience method)
    - Standard getters and setters for all new fields
    - Update `Source(String url, String name)` constructor to default maxPages to 500

    **IngestionStateRepository extensions**:
    Add query methods:
    - `List<IngestionState> findAllBySourceId(UUID sourceId)` -- for deleted page detection (get all indexed pages)
    - `void deleteAllBySourceId(UUID sourceId)` -- for full recrawl reset
    - `void deleteAllBySourceIdAndPageUrlNotIn(UUID sourceId, Collection<String> pageUrls)` -- for orphaned page cleanup. Spring Data derives this from method name.

    Also update the test fixture builder `SourceBuilder` if it exists, adding builder methods for new fields.
  </action>
  <verify>
    `./quality.sh test` passes (migration validated at integration test time). Check Source entity compiles with new fields. Verify IngestionStateRepository method signatures compile.
  </verify>
  <done>V2 migration adds scope columns to sources table. Source entity has scope config fields with helper methods. IngestionStateRepository supports queries needed for incremental crawling (all by source, delete orphans).</done>
</task>

<task type="auto">
  <name>Task 2: CrawlProgressTracker and PageDiscoveryService llms.txt cascade</name>
  <files>
    src/main/java/dev/alexandria/crawl/CrawlProgress.java
    src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
    src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
    src/test/java/dev/alexandria/crawl/CrawlProgressTrackerTest.java
    src/test/java/dev/alexandria/crawl/PageDiscoveryServiceTest.java
    src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
  </files>
  <action>
    **CrawlProgress record** (`crawl/CrawlProgress.java`):
    ```java
    public record CrawlProgress(
        UUID sourceId,
        SourceStatus status,
        int pagesCrawled,
        int pagesSkipped,       // unchanged content hash
        int pagesTotal,         // discovered URLs count (may grow during BFS)
        int errors,
        List<String> errorUrls,
        List<String> filteredUrls,  // URLs excluded by scope
        Instant startedAt
    ) {
        public CrawlProgress {
            errorUrls = errorUrls == null ? List.of() : List.copyOf(errorUrls);
            filteredUrls = filteredUrls == null ? List.of() : List.copyOf(filteredUrls);
        }
    }
    ```

    **CrawlProgressTracker** (`crawl/CrawlProgressTracker.java`):
    - Spring `@Component` (singleton)
    - Internal `ConcurrentHashMap<UUID, CrawlProgress>` for active crawls
    - Methods:
      - `startCrawl(UUID sourceId, int initialTotal)` -- creates initial CrawlProgress with CRAWLING status, 0 crawled/skipped/errors
      - `recordPageCrawled(UUID sourceId)` -- increments pagesCrawled
      - `recordPageSkipped(UUID sourceId)` -- increments pagesSkipped (unchanged hash)
      - `recordError(UUID sourceId, String url)` -- increments errors, adds to errorUrls
      - `recordFiltered(UUID sourceId, String url)` -- adds to filteredUrls
      - `updateTotal(UUID sourceId, int total)` -- updates pagesTotal (BFS discovery grows over time)
      - `completeCrawl(UUID sourceId)` -- sets status to INDEXED
      - `failCrawl(UUID sourceId)` -- sets status to ERROR
      - `getProgress(UUID sourceId)` -- returns `Optional<CrawlProgress>`
      - `removeCrawl(UUID sourceId)` -- cleanup after completion
    - Thread safety: since CrawlProgress is immutable, each update reads current, creates new record with updated fields, puts back. Use `ConcurrentHashMap.compute()` for atomic read-modify-write.
    - Javadoc on class and public methods

    **CrawlProgressTracker tests** (`CrawlProgressTrackerTest`):
    - `startCrawlCreatesInitialProgress` -- verify initial state
    - `recordPageCrawledIncrementsCount`
    - `recordPageSkippedIncrementsCount`
    - `recordErrorAddsUrlToList`
    - `recordFilteredAddsUrlToList`
    - `completeCrawlSetsIndexedStatus`
    - `getProgressReturnsEmptyForUnknownSource`
    - `removeCrawlCleansUpMemory`

    **PageDiscoveryService update**:
    - Add `RestClient.Builder` dependency for HTTP fetching of llms.txt (same pattern as SitemapParser)
    - Update `discoverUrls(String rootUrl)` to implement the priority cascade:
      1. Try `{baseUrl}/llms.txt` via HTTP GET. If found, parse with `LlmsTxtParser.parseUrls()`. If URLs extracted, return with `DiscoveryMethod.LLMS_TXT`.
      2. Try sitemap.xml (existing behavior)
      3. Fall back to LINK_CRAWL (existing behavior)
    - Each level supplements the previous (per user decision): if llms.txt provides URLs AND sitemap provides additional URLs, merge them (union). In practice, return llms.txt URLs as primary set; the CrawlService will discover additional pages via sitemap/links during crawl.
    - Add `LLMS_TXT` to `DiscoveryMethod` enum
    - Also try `{baseUrl}/llms-full.txt`: if found, parse and return with `DiscoveryMethod.LLMS_FULL_TXT`. The CrawlService (Plan 04) will handle ingesting the full content.
    - New `DiscoveryResult` fields: add `String llmsFullContent` (null if not llms-full.txt, contains raw content for direct ingestion)
    - Update `DiscoveryResult` record: `record DiscoveryResult(List<String> urls, DiscoveryMethod method, String llmsFullContent)`
    - The `llmsFullContent` field is only populated when llms-full.txt is available, enabling hybrid ingestion

    **PageDiscoveryService test updates** (`PageDiscoveryServiceTest`):
    - Update existing tests for new DiscoveryResult shape (add null llmsFullContent)
    - Add `llmsTxtFoundReturnsUrls` -- mock HTTP returning llms.txt content, verify URLs extracted
    - Add `llmsFullTxtFoundReturnsContentAndUrls` -- mock HTTP returning llms-full.txt, verify rawContent populated
    - Add `llmsTxtNotFoundFallsToSitemap` -- llms.txt 404, sitemap found
    - Add `noLlmsTxtNoSitemapFallsToLinkCrawl` -- both 404, link crawl returned

    Inject `RestClient.Builder` into PageDiscoveryService constructor. Follow the same pattern as SitemapParser: `restClientBuilder.build()` then `.get().uri(...).retrieve().body(String.class)` with try-catch for 404/errors.

    **IMPORTANT: Update existing CrawlServiceTest.java for new DiscoveryResult shape**:
    The `DiscoveryResult` record changes from `DiscoveryResult(List<String> urls, DiscoveryMethod method)` to `DiscoveryResult(List<String> urls, DiscoveryMethod method, String llmsFullContent)`. CrawlServiceTest.java has 10+ existing calls using the 2-arg constructor. Update ALL existing `new PageDiscoveryService.DiscoveryResult(urls, method)` calls to `new PageDiscoveryService.DiscoveryResult(urls, method, null)` to prevent compile errors. This is a mechanical find-and-replace across the test file.
  </action>
  <verify>
    `./quality.sh test` passes with all new and updated tests green. Run `./gradlew test --tests "dev.alexandria.crawl.CrawlProgressTrackerTest" --tests "dev.alexandria.crawl.PageDiscoveryServiceTest"` specifically.
  </verify>
  <done>CrawlProgressTracker provides thread-safe in-memory progress tracking. PageDiscoveryService implements llms.txt > sitemap > link crawl discovery cascade. CrawlProgress record captures crawl state with error and filtered URL details.</done>
</task>

</tasks>

<verification>
- `./quality.sh test` passes with no regressions
- V2 migration applies cleanly (verified via integration tests if run)
- Source entity has scope config fields that compile and map to DB columns
- IngestionStateRepository has all query methods needed for incremental crawling
- CrawlProgressTracker is thread-safe and all tests pass
- PageDiscoveryService tries llms.txt before sitemap before link crawl
</verification>

<success_criteria>
- V2 Flyway migration creates scope columns on sources table
- Source.toCrawlScope() creates correct CrawlScope from entity fields
- IngestionStateRepository has findAllBySourceId, deleteAllBySourceId, deleteAllBySourceIdAndPageUrlNotIn
- CrawlProgressTracker all 8 tests pass
- PageDiscoveryService discovery cascade verified with 4+ tests
- No regressions in existing ~130 tests
</success_criteria>

<output>
After completion, create `.planning/phases/07-crawl-operations/07-03-SUMMARY.md`
</output>
