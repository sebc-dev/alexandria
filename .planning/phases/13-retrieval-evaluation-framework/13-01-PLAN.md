---
phase: 13-retrieval-evaluation-framework
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/main/java/dev/alexandria/search/eval/RetrievalMetrics.java
  - src/main/java/dev/alexandria/search/eval/QueryType.java
  - src/main/java/dev/alexandria/search/eval/RelevanceJudgment.java
  - src/main/java/dev/alexandria/search/eval/package-info.java
  - src/test/java/dev/alexandria/search/eval/RetrievalMetricsTest.java
autonomous: true
requirements:
  - EVAL-01

must_haves:
  truths:
    - "RetrievalMetrics computes Recall@k correctly for graded relevance (threshold >= 1)"
    - "RetrievalMetrics computes Precision@k correctly"
    - "RetrievalMetrics computes MRR correctly (first relevant result)"
    - "RetrievalMetrics computes NDCG@k correctly with graded relevance (0, 1, 2)"
    - "RetrievalMetrics computes MAP correctly"
    - "RetrievalMetrics computes Hit Rate (binary: at least one relevant in top-k)"
    - "All metrics handle edge cases: empty results, no relevant documents, all relevant"
  artifacts:
    - path: "src/main/java/dev/alexandria/search/eval/RetrievalMetrics.java"
      provides: "IR metric computation"
      contains: "recallAtK"
    - path: "src/main/java/dev/alexandria/search/eval/RelevanceJudgment.java"
      provides: "Graded relevance judgment record"
      contains: "record RelevanceJudgment"
    - path: "src/main/java/dev/alexandria/search/eval/QueryType.java"
      provides: "Query type enum for categorization"
      contains: "enum QueryType"
    - path: "src/test/java/dev/alexandria/search/eval/RetrievalMetricsTest.java"
      provides: "Unit tests for all 6 metrics"
      min_lines: 100
  key_links:
    - from: "src/test/java/dev/alexandria/search/eval/RetrievalMetricsTest.java"
      to: "src/main/java/dev/alexandria/search/eval/RetrievalMetrics.java"
      via: "direct method calls with known inputs/outputs"
      pattern: "RetrievalMetrics\\."
---

<objective>
Implement the RetrievalMetrics class using TDD, providing all 6 standard IR metrics: Recall@k, Precision@k, MRR, NDCG@k, MAP, and Hit Rate.

Purpose: This is the computation core of the evaluation framework. All metrics must be correct before building the golden set or integration test. TDD ensures each metric formula is validated against known test vectors.

Output: Tested, pure-computation RetrievalMetrics class with supporting types (QueryType enum, RelevanceJudgment record).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/main/java/dev/alexandria/search/SearchResult.java
@src/main/java/dev/alexandria/search/package-info.java
</context>

<feature>
  <name>RetrievalMetrics — IR Metric Computation</name>
  <files>
    src/main/java/dev/alexandria/search/eval/RetrievalMetrics.java
    src/main/java/dev/alexandria/search/eval/QueryType.java
    src/main/java/dev/alexandria/search/eval/RelevanceJudgment.java
    src/main/java/dev/alexandria/search/eval/package-info.java
    src/test/java/dev/alexandria/search/eval/RetrievalMetricsTest.java
  </files>
  <behavior>
    RetrievalMetrics is a utility class with static methods computing standard IR metrics.

    **Supporting types to create first:**

    1. `QueryType` enum: FACTUAL, CONCEPTUAL, CODE_LOOKUP, TROUBLESHOOTING

    2. `RelevanceJudgment` record:
       - `chunkId` (String) — identifier of the relevant document chunk
       - `grade` (int) — relevance grade: 0 (not relevant), 1 (partially relevant), 2 (highly relevant)

    3. `package-info.java` for `dev.alexandria.search.eval` with @NullMarked

    **Metrics — inputs and expected outputs:**

    All metrics take: `List<String> retrievedChunkIds` (ordered by rank), `List<RelevanceJudgment> judgments`, `int k`.
    A chunk is "relevant" if its grade >= 1 in the judgments list.

    **Recall@k:** fraction of relevant documents found in top-k results.
    - retrievedIds=["a","b","c"], judgments=[a:2, b:1, d:2], k=3 -> 2/3 = 0.667
    - retrievedIds=["x","y"], judgments=[a:2], k=2 -> 0/1 = 0.0
    - retrievedIds=[], judgments=[a:2], k=5 -> 0/1 = 0.0
    - retrievedIds=["a"], judgments=[], k=1 -> 0.0 (no relevant docs => 0.0 by convention)

    **Precision@k:** fraction of top-k results that are relevant.
    - retrievedIds=["a","b","c"], judgments=[a:2, b:1], k=3 -> 2/3 = 0.667
    - retrievedIds=["a","b","c"], judgments=[a:2, b:1], k=2 -> 2/2 = 1.0
    - retrievedIds=[], judgments=[a:2], k=5 -> 0.0

    **MRR (Mean Reciprocal Rank):** 1/rank of the first relevant result (for a single query).
    - retrievedIds=["x","a","b"], judgments=[a:2, b:1], k=3 -> 1/2 = 0.5
    - retrievedIds=["a","b"], judgments=[a:2], k=2 -> 1/1 = 1.0
    - retrievedIds=["x","y","z"], judgments=[a:2], k=3 -> 0.0

    **NDCG@k:** Normalized Discounted Cumulative Gain with graded relevance.
    - Uses formula: DCG = sum(grade_i / log2(i+1)) for i=0..k-1
    - IDCG = DCG of ideal ranking (all relevant docs sorted by grade desc)
    - NDCG = DCG / IDCG (0.0 if IDCG = 0)
    - retrievedIds=["a","c","b"], judgments=[a:2, b:1, c:0], k=3 -> DCG = 2/1 + 0/1.585 + 1/2 = 2.5, IDCG = 2/1 + 1/1.585 = 2.631, NDCG = 0.950

    **MAP (Mean Average Precision):** Average of precision at each relevant hit position (for a single query, this is "Average Precision").
    - retrievedIds=["a","x","b","y"], judgments=[a:2, b:1], k=4 -> P@1=1/1, P@3=2/3, AP = (1 + 0.667)/2 = 0.833
    - retrievedIds=["x","y"], judgments=[a:2], k=2 -> 0.0

    **Hit Rate:** Binary — 1.0 if at least one relevant document in top-k, else 0.0.
    - retrievedIds=["x","a"], judgments=[a:2], k=2 -> 1.0
    - retrievedIds=["x","y"], judgments=[a:2], k=2 -> 0.0

    **Aggregate method:** `computeAll(retrievedChunkIds, judgments, k)` returns a record/map with all 6 metrics at once. This avoids recomputing relevance lookups.

  </behavior>
  <implementation>
    1. Create `QueryType` enum in `dev.alexandria.search.eval` package.
    2. Create `RelevanceJudgment` record with chunkId and grade fields.
    3. Create `package-info.java` with @NullMarked annotation.
    4. Implement `RetrievalMetrics` as a final utility class (private constructor) with static methods:
       - `recallAtK(List<String> retrievedIds, List<RelevanceJudgment> judgments, int k) -> double`
       - `precisionAtK(...)  -> double`
       - `mrr(...)  -> double`
       - `ndcgAtK(...)  -> double`
       - `averagePrecision(...)  -> double` (MAP for a single query)
       - `hitRate(...)  -> double`
       - `computeAll(...) -> MetricsResult` where MetricsResult is a record with all 6 values
    5. Internally, build a `Map<String, Integer>` from judgments for O(1) grade lookups.
    6. Truncate retrievedIds to min(k, retrievedIds.size()) before computing.
    7. Convention: when there are no relevant documents in judgments (all grades 0 or empty), recall=0.0, precision computed normally, MRR=0.0, NDCG=0.0, MAP=0.0, hitRate=0.0.
  </implementation>
</feature>

<verification>
- `./gradlew test --tests "dev.alexandria.search.eval.RetrievalMetricsTest"` passes
- All 6 metrics tested with at least: happy path, empty results, no relevant docs, all relevant
- `./gradlew spotlessApply && ./gradlew compileJava` passes (NullAway + formatting)
</verification>

<success_criteria>
- RetrievalMetrics computes all 6 IR metrics correctly against known test vectors
- Unit tests cover happy path, edge cases (empty, zero relevant, all relevant), and boundary values
- Code compiles clean with NullAway and Error Prone
</success_criteria>

<output>
After completion, create `.planning/phases/13-retrieval-evaluation-framework/13-01-SUMMARY.md`
</output>
