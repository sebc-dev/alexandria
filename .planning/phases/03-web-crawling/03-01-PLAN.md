---
phase: 03-web-crawling
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - gradle/libs.versions.toml
  - build.gradle.kts
  - src/main/resources/application.yml
  - docker-compose.yml
  - src/main/java/dev/alexandria/crawl/Crawl4AiConfig.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiClient.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiRequest.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiResponse.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiPageResult.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiMarkdown.java
  - src/main/java/dev/alexandria/crawl/Crawl4AiLink.java
  - src/main/java/dev/alexandria/crawl/CrawlResult.java
  - src/integrationTest/java/dev/alexandria/crawl/Crawl4AiClientIT.java
autonomous: true

must_haves:
  truths:
    - "Application can send a crawl request to the Crawl4AI sidecar and receive Markdown content back"
    - "Crawl4AI response includes both raw and filtered (fit) Markdown"
    - "Crawl4AI response includes internal links for URL discovery"
    - "Boilerplate content (nav, footer, sidebar) is stripped via PruningContentFilter"
    - "JavaScript-rendered pages produce Markdown output (Crawl4AI handles JS via Chromium)"
    - "Crawl4AI Docker service has shm_size: 1g to prevent Chromium crashes"
  artifacts:
    - path: "src/main/java/dev/alexandria/crawl/Crawl4AiClient.java"
      provides: "REST client wrapping Spring RestClient for Crawl4AI POST /crawl"
      contains: "restClient.post()"
    - path: "src/main/java/dev/alexandria/crawl/Crawl4AiConfig.java"
      provides: "RestClient bean with configurable timeouts and base URL"
      contains: "@Configuration"
    - path: "src/main/java/dev/alexandria/crawl/Crawl4AiResponse.java"
      provides: "Response DTO matching Crawl4AI schema"
      contains: "Crawl4AiPageResult"
    - path: "src/main/java/dev/alexandria/crawl/CrawlResult.java"
      provides: "Domain DTO for crawl output (url, markdown, links, success)"
    - path: "src/integrationTest/java/dev/alexandria/crawl/Crawl4AiClientIT.java"
      provides: "Integration test proving crawl against real Crawl4AI container"
      contains: "GenericContainer"
  key_links:
    - from: "src/main/java/dev/alexandria/crawl/Crawl4AiClient.java"
      to: "Crawl4AI sidecar POST /crawl"
      via: "Spring RestClient HTTP call"
      pattern: "restClient\\.post\\(\\)"
    - from: "src/main/java/dev/alexandria/crawl/Crawl4AiConfig.java"
      to: "application.yml alexandria.crawl4ai.*"
      via: "@ConfigurationProperties or @Value"
      pattern: "alexandria\\.crawl4ai"
---

<objective>
Crawl4AI REST client with Spring RestClient, request/response DTOs, configuration properties, Docker Compose shm_size fix, and integration test proving single-page crawl against real Crawl4AI container.

Purpose: Establishes the HTTP communication layer between the Spring Boot app and the Crawl4AI sidecar. This is the foundation that Plan 02 (page discovery + orchestration) builds on.
Output: Crawl4AiClient bean that can crawl a single URL and return clean Markdown with internal links.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-web-crawling/03-RESEARCH.md
@.planning/phases/01-foundation-infrastructure/01-01-SUMMARY.md
@.planning/phases/01-foundation-infrastructure/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Crawl4AI dependency, config properties, RestClient bean, and DTOs</name>
  <files>
    gradle/libs.versions.toml
    build.gradle.kts
    src/main/resources/application.yml
    docker-compose.yml
    src/main/java/dev/alexandria/crawl/Crawl4AiConfig.java
    src/main/java/dev/alexandria/crawl/Crawl4AiRequest.java
    src/main/java/dev/alexandria/crawl/Crawl4AiResponse.java
    src/main/java/dev/alexandria/crawl/Crawl4AiPageResult.java
    src/main/java/dev/alexandria/crawl/Crawl4AiMarkdown.java
    src/main/java/dev/alexandria/crawl/Crawl4AiLink.java
    src/main/java/dev/alexandria/crawl/CrawlResult.java
  </files>
  <action>
    **1. Add crawler-commons dependency to version catalog and build file:**

    In `gradle/libs.versions.toml`:
    - Add version: `crawler-commons = "1.6"` in [versions]
    - Add library: `crawler-commons = { module = "com.github.crawler-commons:crawler-commons", version.ref = "crawler-commons" }` in [libraries]

    In `build.gradle.kts`:
    - Add `implementation(libs.crawler.commons)` to dependencies block

    **2. Add Crawl4AI config properties to application.yml:**

    Add under the top level (NOT under spring):
    ```yaml
    alexandria:
      crawl4ai:
        base-url: http://${CRAWL4AI_HOST:localhost}:${CRAWL4AI_PORT:11235}
        connect-timeout-ms: 5000
        read-timeout-ms: 120000
    ```

    **3. Fix Docker Compose crawl4ai service -- add shm_size:**

    In `docker-compose.yml`, add `shm_size: 1g` to the crawl4ai service (prevents Chromium crashes from insufficient /dev/shm). Also add `deploy.resources.limits.memory: 2g` to cap memory usage.

    **4. Create Crawl4AiConfig.java** in `src/main/java/dev/alexandria/crawl/`:

    ```java
    @Configuration
    public class Crawl4AiConfig {

        @Bean
        public RestClient crawl4AiRestClient(
                RestClient.Builder builder,
                @Value("${alexandria.crawl4ai.base-url}") String baseUrl,
                @Value("${alexandria.crawl4ai.connect-timeout-ms}") int connectTimeoutMs,
                @Value("${alexandria.crawl4ai.read-timeout-ms}") int readTimeoutMs) {

            var requestFactory = new SimpleClientHttpRequestFactory();
            requestFactory.setConnectTimeout(Duration.ofMillis(connectTimeoutMs));
            requestFactory.setReadTimeout(Duration.ofMillis(readTimeoutMs));

            return builder
                    .baseUrl(baseUrl)
                    .requestFactory(requestFactory)
                    .defaultHeader(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE)
                    .build();
        }
    }
    ```

    Use `SimpleClientHttpRequestFactory` from `org.springframework.http.client`. Import `java.time.Duration`, `org.springframework.beans.factory.annotation.Value`, `org.springframework.http.HttpHeaders`, `org.springframework.http.MediaType`.

    **5. Create Crawl4AI request/response DTOs** as Java records in `src/main/java/dev/alexandria/crawl/`:

    `Crawl4AiRequest.java` -- record with fields: `List<String> urls`, `Map<String, Object> browser_config`, `Map<String, Object> crawler_config`. Use Jackson snake_case naming (field names already match).

    `Crawl4AiResponse.java` -- record: `boolean success`, `List<Crawl4AiPageResult> results`.

    `Crawl4AiPageResult.java` -- record: `String url`, `boolean success`, `String status_code`, `Crawl4AiMarkdown markdown`, `Map<String, List<Crawl4AiLink>> links`, `String error_message`. Add a convenience method `internalLinkHrefs()` returning `List<String>` that extracts hrefs from `links.getOrDefault("internal", List.of())`.

    `Crawl4AiMarkdown.java` -- record: `String raw_markdown`, `String fit_markdown`, `String markdown_with_citations`, `String references_markdown`.

    `Crawl4AiLink.java` -- record: `String href`, `String text`, `String title`.

    **6. Create CrawlResult.java** -- domain DTO (not tied to Crawl4AI JSON schema):
    ```java
    public record CrawlResult(
        String url,
        String markdown,
        List<String> internalLinks,
        boolean success,
        String errorMessage
    ) {}
    ```

    All DTOs use `@JsonIgnoreProperties(ignoreUnknown = true)` from `com.fasterxml.jackson.annotation` to handle extra fields gracefully. This is important because Crawl4AI may return additional fields not in our DTOs.
  </action>
  <verify>
    `./gradlew build -x integrationTest -x test` compiles successfully. All new classes are in the `dev.alexandria.crawl` package.
  </verify>
  <done>
    - crawler-commons 1.6 resolves in Gradle
    - Crawl4AI config properties defined in application.yml
    - Docker Compose crawl4ai service has shm_size: 1g
    - Crawl4AiConfig creates RestClient bean with configurable timeouts
    - All 6 DTOs compile (Crawl4AiRequest, Crawl4AiResponse, Crawl4AiPageResult, Crawl4AiMarkdown, Crawl4AiLink, CrawlResult)
  </done>
</task>

<task type="auto">
  <name>Task 2: Crawl4AiClient service and integration test</name>
  <files>
    src/main/java/dev/alexandria/crawl/Crawl4AiClient.java
    src/integrationTest/java/dev/alexandria/crawl/Crawl4AiClientIT.java
  </files>
  <action>
    **1. Create Crawl4AiClient.java** in `src/main/java/dev/alexandria/crawl/`:

    ```java
    @Service
    public class Crawl4AiClient {

        private final RestClient restClient;

        public Crawl4AiClient(@Qualifier("crawl4AiRestClient") RestClient restClient) {
            this.restClient = restClient;
        }

        /**
         * Crawl a single URL via Crawl4AI sidecar.
         * Uses PruningContentFilter for boilerplate removal and headless Chromium for JS rendering.
         */
        public CrawlResult crawl(String url) {
            Crawl4AiRequest request = buildRequest(url);

            Crawl4AiResponse response = restClient.post()
                    .uri("/crawl")
                    .body(request)
                    .retrieve()
                    .body(Crawl4AiResponse.class);

            if (response == null || !response.success() || response.results().isEmpty()) {
                return new CrawlResult(url, null, List.of(), false,
                        "Crawl4AI returned no results for " + url);
            }

            Crawl4AiPageResult page = response.results().getFirst();
            if (!page.success()) {
                return new CrawlResult(url, null, List.of(), false, page.error_message());
            }

            // Prefer fit_markdown (boilerplate-removed) over raw_markdown
            String markdown = page.markdown() != null && page.markdown().fit_markdown() != null
                    && !page.markdown().fit_markdown().isBlank()
                    ? page.markdown().fit_markdown()
                    : (page.markdown() != null ? page.markdown().raw_markdown() : null);

            return new CrawlResult(url, markdown, page.internalLinkHrefs(), true, null);
        }

        private Crawl4AiRequest buildRequest(String url) {
            return new Crawl4AiRequest(
                List.of(url),
                Map.of("type", "BrowserConfig", "params", Map.of("headless", true)),
                Map.of("type", "CrawlerRunConfig", "params", Map.of(
                    "cache_mode", "bypass",
                    "word_count_threshold", 50,
                    "excluded_tags", List.of("nav", "footer", "header"),
                    "markdown_generator", Map.of(
                        "type", "DefaultMarkdownGenerator",
                        "params", Map.of(
                            "content_filter", Map.of(
                                "type", "PruningContentFilter",
                                "params", Map.of("threshold", 0.48, "min_word_threshold", 20)
                            )
                        )
                    )
                ))
            );
        }
    }
    ```

    The `@Qualifier("crawl4AiRestClient")` matches the bean name from `Crawl4AiConfig`. Import `org.springframework.beans.factory.annotation.Qualifier`. The `buildRequest` method constructs the nested `{"type": ..., "params": {...}}` format that Crawl4AI's `CrawlerRunConfig.load()` expects. Key config choices:
    - `cache_mode: bypass` -- always fetch fresh content
    - `word_count_threshold: 50` -- skip very short pages
    - `excluded_tags: [nav, footer, header]` -- additional boilerplate removal
    - `PruningContentFilter` with threshold 0.48 -- default recommended threshold for content density analysis

    **2. Create Crawl4AiClientIT.java** integration test in `src/integrationTest/java/dev/alexandria/crawl/`:

    This test starts a real Crawl4AI container via Testcontainers and crawls a known static page. Use `GenericContainer<>` with `unclecode/crawl4ai:0.8.0`, expose port 11235, add `withCreateContainerCmdModifier` for shm_size 1GB, and `Wait.forHttp("/health").forStatusCode(200).withStartupTimeout(Duration.ofSeconds(120))`.

    Use `@DynamicPropertySource` to override `alexandria.crawl4ai.base-url` with the container's mapped host:port.

    The test extends `BaseIntegrationTest` (which provides PostgreSQL for Spring context) and autowires `Crawl4AiClient`.

    Test methods:
    - `crawl_returns_markdown_for_valid_url()`: Crawl `https://example.com` (a simple static page that is always available). Assert: `result.success()` is true, `result.markdown()` is not null and not blank, markdown contains "Example Domain" (the page title).
    - `crawl_returns_internal_links()`: Crawl `https://example.com`. Assert: `result.internalLinks()` is not null (may be empty for example.com since it has few links, but the field should exist).
    - `crawl_returns_failure_for_unreachable_url()`: Crawl `http://localhost:1` (definitely unreachable). Assert: `result.success()` is false.

    Important: The Crawl4AI container is SLOW to start (30-60 seconds for Chromium warmup). Use `@Container` with static field and generous startup timeout. The container must be started before Spring context because `@DynamicPropertySource` runs during context initialization.

    **Testcontainers note:** The crawl4ai container is SEPARATE from the PostgreSQL container in BaseIntegrationTest. Both run simultaneously. The Crawl4AI container is only needed for this test class, so declare it locally (not in BaseIntegrationTest).
  </action>
  <verify>
    `./gradlew build -x integrationTest` compiles successfully (Crawl4AiClient + test compile).
    If Docker is available: `./gradlew integrationTest --tests "dev.alexandria.crawl.Crawl4AiClientIT"` passes.
  </verify>
  <done>
    - Crawl4AiClient bean crawls a URL via Crawl4AI POST /crawl and returns CrawlResult
    - Client prefers fit_markdown (boilerplate-removed) over raw_markdown
    - Client passes PruningContentFilter config for boilerplate stripping
    - Client extracts internal links from response for downstream URL discovery
    - Integration test proves real Crawl4AI container returns Markdown for a valid URL
  </done>
</task>

</tasks>

<verification>
1. `./gradlew build -x integrationTest` compiles all new code without errors
2. New files exist in `src/main/java/dev/alexandria/crawl/` package (7 files)
3. Integration test class exists and compiles
4. `docker-compose.yml` crawl4ai service has `shm_size: 1g`
5. `application.yml` has `alexandria.crawl4ai.*` properties
6. `gradle/libs.versions.toml` has crawler-commons entry
</verification>

<success_criteria>
- The Spring Boot application has a Crawl4AiClient bean that can crawl a single URL via the Crawl4AI REST API
- Crawl responses include clean Markdown (fit_markdown with boilerplate removed) and internal links
- PruningContentFilter is configured for boilerplate removal (nav, footer, sidebar stripping)
- JavaScript-rendered pages are handled by Crawl4AI's Chromium headless browser (configured via BrowserConfig)
- Docker Compose crawl4ai service has shm_size: 1g to prevent Chromium crashes
- Integration test proves end-to-end crawl against real Crawl4AI container
</success_criteria>

<output>
After completion, create `.planning/phases/03-web-crawling/03-01-SUMMARY.md`
</output>
