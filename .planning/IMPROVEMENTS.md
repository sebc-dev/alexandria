# Am√©liorations post-review ‚Äî phase-3/web-crawling

PR: https://github.com/sebc-dev/alexandria/pull/9
Review: 19 fichiers, 45 üü¢, 30 üü°, 0 üî¥
Progression: **15/15 compl√©t√©es**

---

## Robustesse production (priorit√© haute)

- [x] **1. Retry avec backoff sur Crawl4AiClient.crawl()**
  - Fichiers: `Crawl4AiClient.java`, `Crawl4AiConfig.java`, `Crawl4AiProperties.java`, `application.yml`, `build.gradle.kts`, `libs.versions.toml`
  - Ajout√© `spring-retry` + `spring-boot-starter-aop`
  - `@Retryable` sur `crawl()` (retryFor=RestClientException, config via SpEL expressions)
  - `@Recover` m√©thode `recoverCrawl()` pour le fallback gracieux
  - Config: `alexandria.crawl4ai.retry.{max-attempts,delay-ms,multiplier}` (3, 1000, 2.0)

- [x] **2. Limiter la taille des sitemaps t√©l√©charg√©s**
  - Fichiers: `SitemapParser.java`, `Crawl4AiProperties.java`, `application.yml`
  - Nouvelle m√©thode `fetchSitemap()` avec v√©rification de taille apr√®s t√©l√©chargement
  - Limite configurable: `alexandria.crawl4ai.max-sitemap-size-bytes` (d√©faut 10 Mo)
  - Log warning et retour null si d√©pass√©e

- [x] **3. Singleton Container pattern pour les tests IT**
  - Fichiers: `SharedCrawl4AiContainer.java` (nouveau), `Crawl4AiClientIT.java`, `CrawlServiceIT.java`
  - Singleton statique `INSTANCE` avec image centralis√©e en constante
  - M√©thode `baseUrl()` r√©utilis√©e via `@DynamicPropertySource`
  - Les deux IT partagent un seul container au lieu de deux

## Qualit√© de code (priorit√© moyenne)

- [x] **4. @JsonNaming(SnakeCaseStrategy.class) sur les DTOs Crawl4AI**
  - Fichiers: `Crawl4AiRequest.java`, `Crawl4AiMarkdown.java`, `Crawl4AiPageResult.java`, `Crawl4AiClient.java`
  - `browser_config` ‚Üí `browserConfig`, `crawler_config` ‚Üí `crawlerConfig`
  - `raw_markdown` ‚Üí `rawMarkdown`, `fit_markdown` ‚Üí `fitMarkdown`, etc.
  - `status_code` ‚Üí `statusCode`, `error_message` ‚Üí `errorMessage`
  - Supprim√© `@JsonIgnoreProperties` inutile sur `Crawl4AiRequest` (cf. #15)

- [x] **5. @ConfigurationProperties record au lieu de @Value**
  - Fichiers: `Crawl4AiProperties.java` (nouveau), `Crawl4AiConfig.java`
  - Record `Crawl4AiProperties` avec `@ConfigurationProperties(prefix = "alexandria.crawl4ai")`
  - Nested record `Retry` pour la config retry
  - `@EnableConfigurationProperties` sur `Crawl4AiConfig`

- [x] **6. Factoriser normalizeToBase dans UrlNormalizer**
  - Fichiers: `UrlNormalizer.java`, `SitemapParser.java`
  - Nouvelle m√©thode publique `UrlNormalizer.normalizeToBase(String)`
  - `SitemapParser.normalizeToBase()` supprim√©e, remplac√©e par appel √† `UrlNormalizer`
  - `isSameSite()` refactor√© pour utiliser `normalizeToBase()`
  - `effectivePort()` supprim√©e (devenue inutile)

- [x] **7. Extraire helper extractMarkdown() dans Crawl4AiClient**
  - Fichier: `Crawl4AiClient.java`
  - Ternaire imbriqu√©e extraite en m√©thode priv√©e `extractMarkdown(Crawl4AiMarkdown)`
  - Logique claire : null ‚Üí null, fitMarkdown non-blank ‚Üí fitMarkdown, sinon rawMarkdown

- [x] **8. Factoriser setup Testcontainers Crawl4AI**
  - Fichiers: `SharedCrawl4AiContainer.java` (nouveau), `Crawl4AiClientIT.java`, `CrawlServiceIT.java`
  - Combin√© avec #3 ‚Äî image `unclecode/crawl4ai:0.8.0` centralis√©e en constante `IMAGE`
  - Config partag√©e : healthcheck, startup timeout 120s, shmSize 1 Go

## Compl√©tude (priorit√© basse)

- [x] **9. Tri alphab√©tique des query params dans UrlNormalizer**
  - Fichier: `UrlNormalizer.java`
  - `.sorted()` ajout√© dans `filterQueryParams` pour normaliser l'ordre des param√®tres

- [x] **10. Enrichir les tests unitaires UrlNormalizer**
  - Fichier: `UrlNormalizerTest.java`
  - Restructur√© en `@Nested` classes (Normalize, NormalizeToBase, IsSameSite)
  - Ajout√©: port d√©faut omis (80/443), port non-d√©faut conserv√©, null/blank, tri alphab√©tique params, params mixtes tracking+utiles, ports diff√©rents dans isSameSite, URL sans sch√©ma, malformed dans normalizeToBase

- [x] **11. Collecter les pages en √©chec dans CrawlService**
  - Fichiers: `CrawlSiteResult.java` (nouveau), `CrawlService.java`, `CrawlServiceIT.java`
  - Record `CrawlSiteResult(successPages, failedUrls)` remplace `List<CrawlResult>`
  - Les √©checs (crawl failed + exceptions) sont collect√©s dans `failedUrls`

- [x] **12. Crawl concurrent dans CrawlService**
  - Fichiers: `CrawlService.java`, `Crawl4AiProperties.java`, `application.yml`
  - Crawl par batch avec virtual threads (`Executors.newVirtualThreadPerTaskExecutor()`)
  - Concurrence configurable: `alexandria.crawl4ai.crawl-concurrency` (d√©faut 4)
  - Compatible BFS link-crawl (URLs d√©couverts ajout√©s entre les batchs)

## Cosm√©tique (nice to have)

- [x] **13. Regrouper d√©pendances par domaine dans build.gradle.kts**
  - Sections: Spring Boot, AI/Embeddings, Web Crawling, Database, Testing
- [x] **14. Port Crawl4AI optionnel dans docker-compose.yml pour debug local**
  - Port 11235 comment√© avec instruction pour activer
- [x] **15. Retirer @JsonIgnoreProperties de Crawl4AiRequest** (fait avec #4 ‚Äî remplac√© par `@JsonNaming`)
