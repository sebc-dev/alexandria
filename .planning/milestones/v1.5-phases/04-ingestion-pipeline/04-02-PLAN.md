---
phase: 04-ingestion-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/main/java/dev/alexandria/ingestion/IngestionService.java
  - src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedImporter.java
  - src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedRequest.java
  - src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedChunk.java
  - src/integrationTest/java/dev/alexandria/ingestion/IngestionServiceIT.java
  - src/integrationTest/java/dev/alexandria/ingestion/prechunked/PreChunkedImporterIT.java
autonomous: true
must_haves:
  truths:
    - "Crawled pages are chunked, embedded, and stored in EmbeddingStore as searchable TextSegments"
    - "Stored chunks carry all 5 metadata fields matching snake_case convention from Phase 2"
    - "Pre-chunked JSON can be imported with all-or-nothing validation"
    - "Pre-chunked import replaces existing chunks for the same source_url"
    - "Invalid pre-chunked JSON is rejected entirely (no partial import)"
    - "Chunks stored via IngestionService are findable by SearchService hybrid search"
  artifacts:
    - path: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      provides: "Pipeline orchestrator: crawl result -> chunk -> embed -> store"
      contains: "class IngestionService"
    - path: "src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedImporter.java"
      provides: "JSON import with validation and replacement"
      contains: "class PreChunkedImporter"
    - path: "src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedRequest.java"
      provides: "Outer JSON record: source_url + chunks list"
      contains: "record PreChunkedRequest"
    - path: "src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedChunk.java"
      provides: "Inner JSON record: text + 5 metadata fields with Bean Validation"
      contains: "record PreChunkedChunk"
    - path: "src/integrationTest/java/dev/alexandria/ingestion/IngestionServiceIT.java"
      provides: "Integration tests proving ingest -> search roundtrip"
      min_lines: 40
    - path: "src/integrationTest/java/dev/alexandria/ingestion/prechunked/PreChunkedImporterIT.java"
      provides: "Integration tests proving import, validation, and replacement"
      min_lines: 40
  key_links:
    - from: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      to: "src/main/java/dev/alexandria/ingestion/chunking/MarkdownChunker.java"
      via: "calls chunker.chunk() for each crawled page"
      pattern: "chunker\\.chunk"
    - from: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      to: "EmbeddingStore and EmbeddingModel beans"
      via: "embeds chunks and stores via addAll()"
      pattern: "embeddingStore\\.addAll"
    - from: "src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedImporter.java"
      to: "EmbeddingStore"
      via: "removeAll(Filter) for replacement, addAll() for storage"
      pattern: "embeddingStore\\.removeAll"
    - from: "src/integrationTest/java/dev/alexandria/ingestion/IngestionServiceIT.java"
      to: "src/main/java/dev/alexandria/search/SearchService.java"
      via: "verifies ingested chunks are searchable"
      pattern: "searchService\\.search"
---

<objective>
Build the ingestion pipeline orchestrator and pre-chunked JSON importer -- the wiring layer that connects Phase 3's crawled Markdown to Phase 2's searchable EmbeddingStore via Plan 01's MarkdownChunker.

Purpose: This completes the crawl-to-search pipeline. After this plan, crawled documentation becomes searchable. The pre-chunked importer enables AI-assisted chunking workflows as an alternative to automatic heading-based chunking.
Output: IngestionService, PreChunkedImporter, PreChunkedRequest/Chunk DTOs, integration tests proving end-to-end roundtrip.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ingestion-pipeline/04-RESEARCH.md
@.planning/phases/04-ingestion-pipeline/04-CONTEXT.md
@.planning/phases/04-ingestion-pipeline/04-01-SUMMARY.md
@src/main/java/dev/alexandria/config/EmbeddingConfig.java
@src/main/java/dev/alexandria/search/SearchService.java
@src/main/java/dev/alexandria/crawl/CrawlResult.java
@src/main/java/dev/alexandria/crawl/CrawlSiteResult.java
@src/integrationTest/java/dev/alexandria/BaseIntegrationTest.java
@src/integrationTest/java/dev/alexandria/search/HybridSearchIT.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: IngestionService orchestrator and PreChunkedImporter with DTOs</name>
  <files>
    src/main/java/dev/alexandria/ingestion/IngestionService.java
    src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedImporter.java
    src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedRequest.java
    src/main/java/dev/alexandria/ingestion/prechunked/PreChunkedChunk.java
    build.gradle.kts
  </files>
  <action>
**IngestionService** (`dev.alexandria.ingestion`):
- Spring `@Service` with constructor injection of `MarkdownChunker`, `EmbeddingStore<TextSegment>`, `EmbeddingModel`
- `public int ingest(CrawlSiteResult crawlResult)`:
  - Iterates over `crawlResult.successPages()`
  - For each `CrawlResult` page: calls `chunker.chunk(page.markdown(), page.url(), Instant.now().toString())`
  - Converts each `DocumentChunkData` to `TextSegment` with `Metadata` containing all 5 fields (source_url, section_path, content_type, last_updated, language). Language is only added to metadata when non-null.
  - Uses `embeddingModel.embedAll(segments)` for batch embedding (NOT one-by-one -- see Pitfall 7 in research)
  - Calls `embeddingStore.addAll(embeddings, segments)` for batch storage
  - Returns total chunk count across all pages
- `public int ingestPage(String markdown, String sourceUrl, String lastUpdated)`:
  - Convenience method for single-page ingestion (useful for testing and future use)
  - Same chunk -> embed -> store logic as above but for a single page
- Private helper `buildMetadata(DocumentChunkData cd)` creates `Metadata` with all 5 fields. Only adds `language` key when `cd.language() != null` (prose chunks have null language).
- Private helper `storeChunks(List<DocumentChunkData> chunks)` handles the TextSegment conversion + embedding + storage.

**PreChunkedRequest** (`dev.alexandria.ingestion.prechunked`):
- Java record with Jackson + Bean Validation annotations
- Fields: `@NotBlank @JsonProperty("source_url") String sourceUrl`, `@NotEmpty @Valid List<PreChunkedChunk> chunks`
- This is the outer JSON envelope

**PreChunkedChunk** (`dev.alexandria.ingestion.prechunked`):
- Java record with Jackson + Bean Validation annotations
- Fields: `@NotBlank String text`, `@NotBlank @JsonProperty("source_url") String sourceUrl`, `@NotBlank @JsonProperty("section_path") String sectionPath`, `@NotBlank @JsonProperty("content_type") @Pattern(regexp = "prose|code") String contentType`, `@NotBlank @JsonProperty("last_updated") String lastUpdated`, `String language`
- `language` is nullable (null for prose, required string for code -- but this is a soft convention, not validated at the record level)

**PreChunkedImporter** (`dev.alexandria.ingestion.prechunked`):
- Spring `@Service` with constructor injection of `EmbeddingStore<TextSegment>`, `EmbeddingModel`, `Validator` (Jakarta)
- `public int importChunks(PreChunkedRequest request)`:
  1. Validate the request using `Validator.validate(request)`. If any violations, throw `IllegalArgumentException` with violation messages (all-or-nothing per user decision).
  2. Delete existing chunks: `embeddingStore.removeAll(metadataKey("source_url").isEqualTo(request.sourceUrl()))` -- replacement mode per user decision.
  3. Convert each `PreChunkedChunk` to `TextSegment` with `Metadata` (same 5 fields).
  4. Batch embed with `embeddingModel.embedAll(segments)`.
  5. Batch store with `embeddingStore.addAll(embeddings, segments)`.
  6. Return chunk count.
- Use `@Transactional` on `importChunks()` to ensure delete + insert is atomic (see Pitfall 5 in research). If EmbeddingStore.removeAll does not participate in Spring transactions, the test in Task 2 will detect this.

**Build dependency**: If `spring-boot-starter-validation` is not already on the classpath, add it to `build.gradle.kts`: `implementation("org.springframework.boot:spring-boot-starter-validation")`. Check if `@Valid` resolves first -- it might already be available via spring-boot-starter-web.

Compile: `./gradlew compileJava`
  </action>
  <verify>`./gradlew compileJava` succeeds. All 4 new classes compile without errors.</verify>
  <done>IngestionService can orchestrate crawl-to-store pipeline. PreChunkedImporter can validate and import JSON chunks with replacement semantics. Both compile and are ready for integration testing.</done>
</task>

<task type="auto">
  <name>Task 2: Integration tests proving ingest-search roundtrip and pre-chunked import</name>
  <files>
    src/integrationTest/java/dev/alexandria/ingestion/IngestionServiceIT.java
    src/integrationTest/java/dev/alexandria/ingestion/prechunked/PreChunkedImporterIT.java
  </files>
  <action>
**IngestionServiceIT** (extends `BaseIntegrationTest`):
- `@Autowired` IngestionService, SearchService, EmbeddingStore (for cleanup)
- `@BeforeEach`: `embeddingStore.removeAll()` for clean state

Test 1: `ingest_crawl_result_produces_searchable_chunks()`
  - Create a `CrawlSiteResult` with 1 `CrawlResult` page containing Markdown with a heading, prose, and a Java code block:
    ```
    ## Spring Configuration\nSpring Boot auto-configures beans.\n```java\n@Configuration\npublic class AppConfig {}\n```
    ```
  - Call `ingestionService.ingest(crawlResult)`
  - Assert return value >= 2 (at least 1 prose + 1 code chunk)
  - Search via `searchService.search(new SearchRequest("Spring auto-configuration"))`
  - Assert results are not empty
  - Assert at least one result has sourceUrl matching the page URL

Test 2: `ingested_chunks_carry_correct_metadata()`
  - Ingest a single page with known URL and Markdown containing `## Getting Started\nSome text.`
  - Search for "Getting Started"
  - Assert the top result has:
    - `sourceUrl` matching the page URL
    - `sectionPath` containing "getting-started"

Test 3: `ingest_multiple_pages()`
  - Create `CrawlSiteResult` with 2 pages (different URLs, different content)
  - Ingest and verify chunk count covers both pages
  - Search for content from each page and verify both are found

Test 4: `ingest_page_with_only_code_blocks()`
  - Ingest a page with `## Snippet\n```bash\necho hello\n```\n`
  - Verify at least 1 chunk stored (code chunk, no empty prose chunk)

**PreChunkedImporterIT** (extends `BaseIntegrationTest`):
- `@Autowired` PreChunkedImporter, SearchService, EmbeddingStore

Test 1: `import_valid_chunks_makes_them_searchable()`
  - Create `PreChunkedRequest` with 2 chunks (1 prose, 1 code) and a source_url
  - Call `importer.importChunks(request)`
  - Search for the prose chunk content via SearchService
  - Assert it appears in results with correct sourceUrl

Test 2: `import_replaces_existing_chunks_for_same_source_url()`
  - Import chunks with source_url "https://example.com/docs"
  - Import different chunks with the SAME source_url
  - Search for OLD content -- should NOT appear
  - Search for NEW content -- should appear
  - This validates the removeAll + addAll replacement semantics

Test 3: `import_rejects_invalid_chunks_entirely()`
  - Create `PreChunkedRequest` with 1 valid chunk and 1 invalid chunk (e.g., blank text)
  - Call `importer.importChunks(request)` -- expect `IllegalArgumentException`
  - Verify NO chunks were stored (all-or-nothing: search for valid chunk content returns nothing)

Test 4: `import_rejects_invalid_content_type()`
  - Create `PreChunkedRequest` with a chunk having `content_type="invalid"`
  - Expect `IllegalArgumentException` from validation

Run: `./gradlew integrationTest --tests "dev.alexandria.ingestion.*"`
  </action>
  <verify>`./gradlew integrationTest --tests "dev.alexandria.ingestion.*"` -- all 8 integration tests pass.</verify>
  <done>IngestionService ingests crawled Markdown into searchable chunks with correct metadata. PreChunkedImporter validates, replaces, and stores JSON chunks. Both roundtrip-verified against real pgvector + hybrid search.</done>
</task>

</tasks>

<verification>
```bash
# All unit tests (including Plan 01)
./gradlew test

# All integration tests (including Plan 02)
./gradlew integrationTest --tests "dev.alexandria.ingestion.*"

# Full check (unit + integration + quality gates)
./gradlew check
```

Phase 4 success criteria verification:
1. SC1 (heading chunking): Verified by MarkdownChunkerTest (Plan 01)
2. SC2 (configurable overlap): OVERRIDDEN by user decision -- no overlap implemented (heading path provides context)
3. SC3 (metadata): Verified by IngestionServiceIT Test 2 (metadata roundtrip)
4. SC4 (code extraction): Verified by MarkdownChunkerTest code block cases + IngestionServiceIT Test 1
5. SC5 (pre-chunked import): Verified by PreChunkedImporterIT Tests 1-4
6. SC6 (end-to-end): Verified by IngestionServiceIT Test 1 (ingest -> search roundtrip with real pgvector)
</verification>

<success_criteria>
- IngestionService.ingest(CrawlSiteResult) produces searchable chunks in EmbeddingStore
- SearchService.search() finds chunks stored by IngestionService with correct metadata
- PreChunkedImporter validates JSON input (all-or-nothing rejection)
- PreChunkedImporter replaces existing chunks for the same source_url
- All 8 integration tests pass against real pgvector via Testcontainers
- ./gradlew check passes (unit + integration + quality gates)
</success_criteria>

<output>
After completion, create `.planning/phases/04-ingestion-pipeline/04-02-SUMMARY.md`
</output>
