---
phase: 09-source-management-completion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/main/java/dev/alexandria/ingestion/IngestionService.java
  - src/main/java/dev/alexandria/document/DocumentChunkRepository.java
  - src/main/java/dev/alexandria/crawl/CrawlService.java
  - src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
  - src/main/resources/db/migration/V4__cleanup_orphan_chunks.sql
  - src/test/java/dev/alexandria/ingestion/IngestionServiceTest.java
  - src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
  - src/test/java/dev/alexandria/crawl/CrawlProgressTrackerTest.java
autonomous: true
requirements:
  - SRC-03
  - SRC-01

must_haves:
  truths:
    - "All document_chunks rows ingested after this phase have source_id populated"
    - "CrawlProgressTracker supports crawl cancellation via cancelCrawl/isCancelled methods"
    - "Removing a source via remove_source deletes all associated chunks (cascade delete functional)"
    - "After startup, no document_chunks rows with NULL source_id exist (orphans cleaned up)"
  artifacts:
    - path: "src/main/java/dev/alexandria/document/DocumentChunkRepository.java"
      provides: "updateSourceIdBatch native query, countBySourceId query, countBySourceIdGroupedByContentType query"
      contains: "updateSourceIdBatch"
    - path: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      provides: "storeChunks with sourceId param, ingestPage with sourceId overload"
      contains: "updateSourceIdBatch"
    - path: "src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java"
      provides: "cancelCrawl and isCancelled methods"
      contains: "cancelCrawl"
    - path: "src/main/resources/db/migration/V4__cleanup_orphan_chunks.sql"
      provides: "DELETE FROM document_chunks WHERE source_id IS NULL"
  key_links:
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      via: "ingestPage with sourceId parameter"
      pattern: "ingestPage.*sourceId"
    - from: "src/main/java/dev/alexandria/ingestion/IngestionService.java"
      to: "src/main/java/dev/alexandria/document/DocumentChunkRepository.java"
      via: "updateSourceIdBatch after embeddingStore.addAll"
      pattern: "updateSourceIdBatch"
---

<objective>
Fix source_id FK population so document_chunks rows are properly linked to their source, add cancellation support to CrawlProgressTracker, and clean up historical orphan chunks via Flyway migration.

Purpose: This is the foundation fix that unblocks cascade delete (remove_source), accurate chunk counts, and proper source-chunk relationships. Without source_id populated, ON DELETE CASCADE is inert and chunk counts are inaccurate.

Output: IngestionService populates source_id on every stored chunk, CrawlProgressTracker supports cancellation, orphan chunks cleaned up.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-source-management-completion/09-RESEARCH.md
@src/main/java/dev/alexandria/ingestion/IngestionService.java
@src/main/java/dev/alexandria/document/DocumentChunkRepository.java
@src/main/java/dev/alexandria/crawl/CrawlService.java
@src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: source_id FK population and ingestion pipeline threading</name>
  <files>
    src/main/java/dev/alexandria/document/DocumentChunkRepository.java
    src/main/java/dev/alexandria/ingestion/IngestionService.java
    src/main/java/dev/alexandria/crawl/CrawlService.java
    src/main/resources/db/migration/V4__cleanup_orphan_chunks.sql
  </files>
  <action>
    **DocumentChunkRepository** -- Add three new native queries:

    1. `updateSourceIdBatch(UUID sourceId, String[] embeddingIds)` -- Batch UPDATE setting source_id on chunks by embedding_id array:
       ```sql
       UPDATE document_chunks SET source_id = :sourceId
       WHERE embedding_id = ANY(CAST(:embeddingIds AS uuid[]))
       ```
       Use @Modifying @Transactional @Query(nativeQuery=true). Parameter types: `@Param("sourceId") UUID`, `@Param("embeddingIds") String[]`.

    2. `countBySourceId(UUID sourceId)` -- Simple COUNT(*) for chunk deletion feedback:
       ```sql
       SELECT COUNT(*) FROM document_chunks WHERE source_id = :sourceId
       ```

    3. `countBySourceIdGroupedByContentType(UUID sourceId)` -- COUNT with content_type breakdown from JSONB metadata:
       ```sql
       SELECT COALESCE(metadata->>'content_type', 'unknown') AS content_type, COUNT(*) AS cnt
       FROM document_chunks WHERE source_id = :sourceId
       GROUP BY metadata->>'content_type'
       ```
       Return type: `List<Object[]>`.

    **IngestionService** -- Thread sourceId through the pipeline:

    1. Add `DocumentChunkRepository` as a constructor dependency (new field).
    2. Add a new `ingestPage` overload that accepts `UUID sourceId` as first parameter (6 params total: sourceId, markdown, sourceUrl, lastUpdated, version, sourceName). The existing 3-arg and 5-arg overloads delegate to this new one with `sourceId = null`.
    3. Modify `storeChunks(List<DocumentChunkData> chunks)` to accept `UUID sourceId` as a second parameter: `storeChunks(List<DocumentChunkData> chunks, UUID sourceId)`.
    4. After each `embeddingStore.addAll(embeddings, batch)` call, capture the returned `List<String> ids`. If `sourceId != null`, call `documentChunkRepository.updateSourceIdBatch(sourceId, ids.toArray(String[]::new))`.
    5. IMPORTANT: Do NOT wrap storeChunks in @Transactional. LangChain4j uses its own JDBC connection. The UPDATE is its own transaction via the @Transactional on the repository method.

    **CrawlService** -- Pass sourceId to ingestPage:

    1. In `ingestIncremental()`: pass `sourceId` as the first argument to `ingestionService.ingestPage(sourceId, markdown, ...)`.
    2. In the llms-full.txt ingestion call (line ~94): pass `sourceId` as the first argument.
    3. Both call sites already have `sourceId` available in scope.

    **Flyway V4 migration** (`V4__cleanup_orphan_chunks.sql`):
    ```sql
    -- Clean up historical orphan chunks with no source association.
    -- After this migration, the source_id FK population fix in IngestionService
    -- ensures all new chunks get source_id populated during ingestion.
    DELETE FROM document_chunks WHERE source_id IS NULL;
    ```

    **IMPORTANT ANTI-PATTERNS to avoid:**
    - Do NOT annotate IngestionService.storeChunks() with @Transactional
    - Do NOT try to make embeddingStore.addAll() + updateSourceIdBatch() atomic
    - Accept best-effort: if UPDATE fails, chunks exist with NULL source_id (no worse than current state)
  </action>
  <verify>
    - `./quality.sh test` passes (unit tests)
    - `./quality.sh integration` passes (integration tests with real DB verify V4 migration runs)
    - Grep: `grep -r "updateSourceIdBatch" src/main/java/` shows calls in both IngestionService and DocumentChunkRepository
  </verify>
  <done>
    - DocumentChunkRepository has updateSourceIdBatch, countBySourceId, countBySourceIdGroupedByContentType queries
    - IngestionService.storeChunks captures addAll return value and calls updateSourceIdBatch when sourceId is non-null
    - CrawlService passes sourceId to ingestPage at both call sites (llms-full.txt and incremental)
    - V4 migration deletes orphan chunks on startup
    - All existing tests pass (backward-compatible 3-arg and 5-arg ingestPage overloads preserved)
  </done>
</task>

<task type="auto">
  <name>Task 2: CrawlProgressTracker cancellation + unit tests for pipeline changes</name>
  <files>
    src/main/java/dev/alexandria/crawl/CrawlProgressTracker.java
    src/main/java/dev/alexandria/crawl/CrawlService.java
    src/test/java/dev/alexandria/ingestion/IngestionServiceTest.java
    src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
    src/test/java/dev/alexandria/crawl/CrawlProgressTrackerTest.java
  </files>
  <action>
    **CrawlProgressTracker** -- Add cancellation support:

    1. Add a `ConcurrentHashMap.newKeySet()` field: `private final Set<UUID> cancelledCrawls`.
    2. Add `cancelCrawl(UUID sourceId)` -- adds sourceId to cancelledCrawls set.
    3. Add `isCancelled(UUID sourceId)` -- returns `cancelledCrawls.contains(sourceId)`.
    4. In `completeCrawl()` and `failCrawl()` methods, add `cancelledCrawls.remove(sourceId)` to clean up.
    5. In `removeCrawl()`, also add `cancelledCrawls.remove(sourceId)`.

    **CrawlService** -- Add cancellation check in crawl loop:

    In `crawlSite()`, at the start of the while-loop body (line ~118), add:
    ```java
    if (sourceId != null && progressTracker.isCancelled(sourceId)) {
        log.info("Crawl cancelled for source {}", sourceId);
        break;
    }
    ```
    This check goes right after `while (!queue.isEmpty() && results.size() < scope.maxPages()) {`.

    **Unit Tests:**

    *CrawlProgressTrackerTest* (new or extend existing):
    - `cancelCrawlMakesCrawlCancelled` -- startCrawl, cancelCrawl, assert isCancelled returns true
    - `isCancelledReturnsFalseForUncancelledCrawl` -- startCrawl without cancel, assert false
    - `completeCrawlClearsCancellationFlag` -- cancel then complete, assert isCancelled returns false
    - `removeCrawlClearsCancellationFlag` -- cancel then remove, assert isCancelled returns false

    *IngestionServiceTest* (update existing):
    - Add `@Mock DocumentChunkRepository documentChunkRepository` as a new field alongside the existing @Mock fields (chunker, embeddingStore, embeddingModel, ingestionStateRepository). Do NOT switch to manual construction -- @InjectMocks will automatically inject all 5 mocks via the 5-arg constructor.
    - Add test: `storeChunksCallsUpdateSourceIdBatchWhenSourceIdProvided` -- mock embeddingStore.addAll to return ["id1","id2"], call ingestPage with sourceId, verify documentChunkRepository.updateSourceIdBatch called with correct sourceId and ids
    - Add test: `storeChunksSkipsSourceIdUpdateWhenSourceIdNull` -- call ingestPage without sourceId (3-arg overload), verify updateSourceIdBatch is never called

    *CrawlServiceTest* (update existing):
    - Add test: `crawlSiteBreaksOnCancellation` -- set up progressTracker.isCancelled to return true, verify crawl loop exits early (results list is empty or shorter than expected)
    - Update existing tests for new ingestPage 6-arg overload with sourceId parameter (add `any(UUID.class)` or explicit sourceId matcher to ingestPage stubs and verifications)
  </action>
  <verify>
    - `./quality.sh test` passes with new tests
    - `./quality.sh spotbugs` shows no new findings
    - New CrawlProgressTracker tests verify cancellation lifecycle
    - IngestionService tests verify source_id update behavior
  </verify>
  <done>
    - CrawlProgressTracker has cancelCrawl/isCancelled methods with cleanup in completeCrawl/failCrawl/removeCrawl
    - CrawlService checks isCancelled at start of crawl loop iteration
    - Unit tests verify: source_id batch update with non-null sourceId, no update with null sourceId, cancellation breaks crawl loop, cancellation flag lifecycle
    - All quality gates pass
  </done>
</task>

</tasks>

<verification>
- `./quality.sh test` -- all unit tests pass including new ones
- `./quality.sh integration` -- integration tests pass (V4 migration runs, source_id FK is testable)
- `./quality.sh spotbugs` -- no new findings
- Grep for `updateSourceIdBatch` in IngestionService confirms wiring
- Grep for `isCancelled` in CrawlService confirms cancellation check
</verification>

<success_criteria>
- source_id is populated on document_chunks rows during ingestion
- CrawlProgressTracker supports cancellation lifecycle
- CrawlService respects cancellation flag
- Orphan chunks deleted by V4 migration
- All existing tests pass with no regressions
- SRC-01 (add_source): already functional from Phase 7, verified by existing McpToolServiceTest addSource* tests -- no new code needed in this plan
</success_criteria>

<output>
After completion, create `.planning/phases/09-source-management-completion/09-01-SUMMARY.md`
</output>
