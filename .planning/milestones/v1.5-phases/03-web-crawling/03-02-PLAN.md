---
phase: 03-web-crawling
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/main/java/dev/alexandria/crawl/SitemapParser.java
  - src/main/java/dev/alexandria/crawl/UrlNormalizer.java
  - src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
  - src/main/java/dev/alexandria/crawl/CrawlService.java
  - src/test/java/dev/alexandria/crawl/UrlNormalizerTest.java
  - src/integrationTest/java/dev/alexandria/crawl/CrawlServiceIT.java
autonomous: true

must_haves:
  truths:
    - "System recursively crawls a documentation site from a root URL, following internal links to discover pages"
    - "System checks for sitemap.xml and uses it for page discovery when available"
    - "System falls back to recursive link crawling when no sitemap.xml exists"
    - "URLs are normalized to prevent duplicate crawling (trailing slashes, fragments, query params)"
    - "Crawl produces a list of CrawlResult objects with clean Markdown for each discovered page"
  artifacts:
    - path: "src/main/java/dev/alexandria/crawl/SitemapParser.java"
      provides: "Sitemap.xml parser using crawler-commons"
      contains: "SiteMapParser"
    - path: "src/main/java/dev/alexandria/crawl/UrlNormalizer.java"
      provides: "URL normalization for deduplication"
      contains: "normalize"
    - path: "src/main/java/dev/alexandria/crawl/PageDiscoveryService.java"
      provides: "Sitemap-first URL discovery with link crawl fallback"
      contains: "discoverUrls"
    - path: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      provides: "Crawl orchestrator: discover URLs then crawl each page"
      contains: "crawlSite"
    - path: "src/test/java/dev/alexandria/crawl/UrlNormalizerTest.java"
      provides: "Unit tests for URL normalization edge cases"
    - path: "src/integrationTest/java/dev/alexandria/crawl/CrawlServiceIT.java"
      provides: "Integration test proving multi-page crawl orchestration"
  key_links:
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/crawl/Crawl4AiClient.java"
      via: "Injected dependency, calls crawl() per URL"
      pattern: "crawl4AiClient\\.crawl"
    - from: "src/main/java/dev/alexandria/crawl/CrawlService.java"
      to: "src/main/java/dev/alexandria/crawl/PageDiscoveryService.java"
      via: "Injected dependency, calls discoverUrls()"
      pattern: "pageDiscoveryService\\.discoverUrls"
    - from: "src/main/java/dev/alexandria/crawl/PageDiscoveryService.java"
      to: "src/main/java/dev/alexandria/crawl/SitemapParser.java"
      via: "Injected dependency, tries sitemap first"
      pattern: "sitemapParser\\.discoverFromSitemap"
---

<objective>
Page discovery (sitemap.xml + recursive link following), URL normalization, and crawl orchestration that ties together Crawl4AiClient from Plan 01 into a complete site crawling capability.

Purpose: Delivers the complete crawl pipeline: give it a root URL, get back clean Markdown for every discovered page. This is the raw material that Phase 4 (Ingestion Pipeline) transforms into searchable chunks.
Output: CrawlService bean that crawls an entire documentation site and returns a list of CrawlResult objects.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-web-crawling/03-RESEARCH.md
@.planning/phases/03-web-crawling/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: SitemapParser, UrlNormalizer, and PageDiscoveryService</name>
  <files>
    src/main/java/dev/alexandria/crawl/SitemapParser.java
    src/main/java/dev/alexandria/crawl/UrlNormalizer.java
    src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
    src/test/java/dev/alexandria/crawl/UrlNormalizerTest.java
  </files>
  <action>
    **1. Create UrlNormalizer.java** in `src/main/java/dev/alexandria/crawl/`:

    A utility class (no Spring bean needed -- static methods) that normalizes URLs for deduplication:
    - Remove fragments (`#section`)
    - Remove common tracking query params (utm_*, ref, source) -- keep meaningful params
    - Normalize trailing slashes: remove trailing slash unless URL is just the domain root (e.g., `https://docs.example.com/` keeps it, `https://docs.example.com/guide/` removes it)
    - Lowercase the scheme and host (path is case-sensitive)
    - Use `java.net.URI` for parsing

    Method signature: `public static String normalize(String url)` -- returns normalized URL string. Returns input unchanged if URL is malformed (log a warning, don't throw).

    Also: `public static boolean isSameSite(String rootUrl, String candidateUrl)` -- returns true if candidateUrl has the same scheme+host+port as rootUrl. Used to filter out external links.

    **2. Create UrlNormalizerTest.java** unit test in `src/test/java/dev/alexandria/crawl/`:

    Test cases:
    - `normalize_removes_fragment()`: `https://docs.example.com/guide#section` -> `https://docs.example.com/guide`
    - `normalize_removes_trailing_slash()`: `https://docs.example.com/guide/` -> `https://docs.example.com/guide`
    - `normalize_keeps_root_trailing_slash()`: `https://docs.example.com/` -> `https://docs.example.com/`
    - `normalize_lowercases_host()`: `https://Docs.Example.COM/Guide` -> `https://docs.example.com/Guide`
    - `normalize_removes_tracking_params()`: `https://docs.example.com/guide?utm_source=x&ref=y` -> `https://docs.example.com/guide`
    - `normalize_keeps_meaningful_params()`: `https://docs.example.com/api?version=3` -> `https://docs.example.com/api?version=3`
    - `normalize_handles_malformed_url()`: `not-a-url` returns `not-a-url` (no exception)
    - `isSameSite_matches_same_host()`: rootUrl `https://docs.spring.io/guide`, candidate `https://docs.spring.io/other` -> true
    - `isSameSite_rejects_different_host()`: rootUrl `https://docs.spring.io/guide`, candidate `https://github.com/spring` -> false
    - `isSameSite_rejects_different_scheme()`: rootUrl `https://docs.spring.io`, candidate `http://docs.spring.io` -> false

    **3. Create SitemapParser.java** in `src/main/java/dev/alexandria/crawl/`:

    A Spring `@Component` that uses `crawler-commons` `SiteMapParser` to parse sitemap.xml.

    ```java
    @Component
    public class SitemapParser {

        private static final Logger log = LoggerFactory.getLogger(SitemapParser.class);
        private final RestClient.Builder restClientBuilder;

        public SitemapParser(RestClient.Builder restClientBuilder) {
            this.restClientBuilder = restClientBuilder;
        }

        /**
         * Try to discover URLs from sitemap.xml at well-known locations.
         * Returns empty list if no sitemap found or unparseable.
         */
        public List<String> discoverFromSitemap(String rootUrl) {
            String baseUrl = normalizeToBase(rootUrl);
            List<String> candidates = List.of(
                baseUrl + "/sitemap.xml",
                baseUrl + "/sitemap_index.xml"
            );

            RestClient httpClient = restClientBuilder
                    .defaultHeader(HttpHeaders.ACCEPT, "*/*")
                    .build();

            for (String sitemapUrl : candidates) {
                try {
                    byte[] content = httpClient.get()
                            .uri(sitemapUrl)
                            .retrieve()
                            .body(byte[].class);
                    if (content == null || content.length == 0) continue;

                    crawlercommons.sitemaps.SiteMapParser parser =
                            new crawlercommons.sitemaps.SiteMapParser(false);
                    AbstractSiteMap result = parser.parseSiteMap(content, new URL(sitemapUrl));

                    if (result instanceof SiteMapIndex index) {
                        return index.getSitemaps().stream()
                                .map(AbstractSiteMap::getUrl)
                                .map(URL::toString)
                                .flatMap(url -> parseSingleSitemap(httpClient, url).stream())
                                .toList();
                    } else if (result instanceof SiteMap siteMap) {
                        return extractUrls(siteMap);
                    }
                } catch (Exception e) {
                    log.debug("Sitemap not available at {}: {}", sitemapUrl, e.getMessage());
                }
            }
            return List.of();
        }
    }
    ```

    The `normalizeToBase(rootUrl)` helper extracts scheme + host (+ port if non-default) from the URL. For example, `https://docs.spring.io/boot/reference/` -> `https://docs.spring.io`.

    The `parseSingleSitemap(httpClient, url)` helper fetches and parses a single sitemap file (for sitemap index entries).

    The `extractUrls(SiteMap)` helper extracts URL strings from a SiteMap object.

    Import from crawler-commons: `crawlercommons.sitemaps.AbstractSiteMap`, `crawlercommons.sitemaps.SiteMap`, `crawlercommons.sitemaps.SiteMapIndex`, `crawlercommons.sitemaps.SiteMapURL`.

    **4. Create PageDiscoveryService.java** in `src/main/java/dev/alexandria/crawl/`:

    A Spring `@Service` that coordinates URL discovery:

    ```java
    @Service
    public class PageDiscoveryService {

        private final SitemapParser sitemapParser;

        public PageDiscoveryService(SitemapParser sitemapParser) {
            this.sitemapParser = sitemapParser;
        }

        /**
         * Discover URLs for a site. Tries sitemap.xml first, falls back to empty
         * (caller will use recursive link crawling).
         *
         * @return discovered URLs, or empty list (caller should seed with rootUrl for link crawling)
         */
        public DiscoveryResult discoverUrls(String rootUrl) {
            List<String> sitemapUrls = sitemapParser.discoverFromSitemap(rootUrl);
            if (!sitemapUrls.isEmpty()) {
                // Filter to same-site URLs and normalize
                List<String> filtered = sitemapUrls.stream()
                        .filter(url -> UrlNormalizer.isSameSite(rootUrl, url))
                        .map(UrlNormalizer::normalize)
                        .distinct()
                        .toList();
                return new DiscoveryResult(filtered, DiscoveryMethod.SITEMAP);
            }
            return new DiscoveryResult(List.of(), DiscoveryMethod.LINK_CRAWL);
        }

        public enum DiscoveryMethod { SITEMAP, LINK_CRAWL }

        public record DiscoveryResult(List<String> urls, DiscoveryMethod method) {}
    }
    ```

    The `DiscoveryResult` tells the caller whether URLs came from sitemap (complete, no link following needed) or if it should fall back to recursive link crawling.
  </action>
  <verify>
    `./gradlew build -x integrationTest` passes. Unit tests in UrlNormalizerTest pass: `./gradlew test --tests "dev.alexandria.crawl.UrlNormalizerTest"`.
  </verify>
  <done>
    - UrlNormalizer handles fragment removal, trailing slash normalization, host lowercasing, tracking param removal
    - All UrlNormalizer unit tests pass
    - SitemapParser discovers URLs from sitemap.xml and sitemap index files
    - PageDiscoveryService tries sitemap first, returns DiscoveryResult with method indicator
    - All classes compile and unit tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: CrawlService orchestrator and integration test</name>
  <files>
    src/main/java/dev/alexandria/crawl/CrawlService.java
    src/integrationTest/java/dev/alexandria/crawl/CrawlServiceIT.java
  </files>
  <action>
    **1. Create CrawlService.java** in `src/main/java/dev/alexandria/crawl/`:

    The crawl orchestrator that ties everything together. Uses BFS (breadth-first search) for URL processing.

    ```java
    @Service
    public class CrawlService {

        private static final Logger log = LoggerFactory.getLogger(CrawlService.class);

        private final Crawl4AiClient crawl4AiClient;
        private final PageDiscoveryService pageDiscoveryService;

        public CrawlService(Crawl4AiClient crawl4AiClient,
                            PageDiscoveryService pageDiscoveryService) {
            this.crawl4AiClient = crawl4AiClient;
            this.pageDiscoveryService = pageDiscoveryService;
        }

        /**
         * Crawl a documentation site from a root URL.
         * Tries sitemap.xml first for URL discovery. If no sitemap, falls back to
         * BFS link crawling starting from rootUrl.
         *
         * @param rootUrl the starting URL for the crawl
         * @param maxPages maximum number of pages to crawl (safety limit)
         * @return list of crawl results for each successfully crawled page
         */
        public List<CrawlResult> crawlSite(String rootUrl, int maxPages) {
            PageDiscoveryService.DiscoveryResult discovery = pageDiscoveryService.discoverUrls(rootUrl);

            LinkedHashSet<String> queue = new LinkedHashSet<>();
            Set<String> visited = new HashSet<>();
            List<CrawlResult> results = new ArrayList<>();
            boolean useLinkDiscovery = discovery.method() == PageDiscoveryService.DiscoveryMethod.LINK_CRAWL;

            if (!discovery.urls().isEmpty()) {
                queue.addAll(discovery.urls());
            } else {
                queue.add(UrlNormalizer.normalize(rootUrl));
            }

            while (!queue.isEmpty() && results.size() < maxPages) {
                String url = queue.iterator().next();
                queue.remove(url);

                String normalized = UrlNormalizer.normalize(url);
                if (!visited.add(normalized)) continue;

                log.info("Crawling [{}/{}]: {}", results.size() + 1, maxPages, normalized);

                try {
                    CrawlResult result = crawl4AiClient.crawl(normalized);
                    if (result.success()) {
                        results.add(result);
                        // Only follow links if we are in link-crawl mode (no sitemap found)
                        if (useLinkDiscovery) {
                            result.internalLinks().stream()
                                    .map(UrlNormalizer::normalize)
                                    .filter(link -> UrlNormalizer.isSameSite(rootUrl, link))
                                    .filter(link -> !visited.contains(link))
                                    .forEach(queue::add);
                        }
                    } else {
                        log.warn("Failed to crawl {}: {}", normalized, result.errorMessage());
                    }
                } catch (Exception e) {
                    log.error("Error crawling {}: {}", normalized, e.getMessage());
                }
            }

            log.info("Crawl complete: {} pages crawled from {}", results.size(), rootUrl);
            return results;
        }
    }
    ```

    Key design choices:
    - `maxPages` parameter prevents runaway crawls (safety limit, caller decides the cap)
    - `LinkedHashSet` for queue preserves insertion order (BFS)
    - URL normalization on BOTH queue additions AND visited checks ensures dedup
    - Link following ONLY happens in LINK_CRAWL mode (when no sitemap was found). When sitemap provides the URL list, we trust it and don't follow links from crawled pages.
    - Sequential page crawling (no concurrency) -- keeps Crawl4AI sidecar stable, avoids Chromium OOM. Concurrency can be added later if needed.
    - Each page crawl failure is logged but doesn't stop the overall crawl.

    **2. Create CrawlServiceIT.java** integration test in `src/integrationTest/java/dev/alexandria/crawl/`:

    This test extends `BaseIntegrationTest` and uses the same Crawl4AI Testcontainer pattern as Crawl4AiClientIT. Declare a static `GenericContainer` for Crawl4AI with the same configuration (shm_size, health wait, port 11235). Use `@DynamicPropertySource` for `alexandria.crawl4ai.base-url`.

    Autowire `CrawlService`.

    Test methods:
    - `crawlSite_crawls_at_least_one_page()`: Call `crawlService.crawlSite("https://example.com", 5)`. Assert: result list is not empty, first result has success=true, first result has non-blank markdown.
    - `crawlSite_respects_maxPages_limit()`: Call `crawlService.crawlSite("https://example.com", 1)`. Assert: result list has size <= 1.
    - `crawlSite_normalizes_urls_for_dedup()`: Call `crawlService.crawlSite("https://example.com/", 5)` (with trailing slash). Assert: no duplicate URLs in results. Verify by collecting all result URLs and checking `new HashSet<>(urls).size() == urls.size()`.

    **Note on test scope:** These tests use example.com which is a single-page site, so they primarily test the orchestration flow (discovery attempt -> crawl -> results). Full multi-page recursive crawl testing would require a more complex test fixture (e.g., a local HTTP server) which is out of scope for this plan. The integration test proves the wiring works end-to-end.

    **Important:** If both Crawl4AiClientIT and CrawlServiceIT declare the same static Crawl4AI container, they may conflict. To avoid this, extract the Crawl4AI container declaration into a shared helper or use a single `@Container` static field pattern. The simplest approach: have both IT classes declare their own container (Testcontainers handles parallel container lifecycle per test class). This is fine since integration tests run sequentially by default.
  </action>
  <verify>
    `./gradlew build -x integrationTest` compiles successfully.
    If Docker is available: `./gradlew integrationTest --tests "dev.alexandria.crawl.CrawlServiceIT"` passes.
  </verify>
  <done>
    - CrawlService.crawlSite() orchestrates full site crawl with sitemap-first discovery and BFS link fallback
    - URL normalization prevents duplicate page crawls
    - maxPages parameter caps crawl size (safety limit)
    - Link following only active when no sitemap was found
    - Integration test proves end-to-end crawl orchestration against real Crawl4AI container
  </done>
</task>

</tasks>

<verification>
1. `./gradlew build -x integrationTest` compiles all new code
2. `./gradlew test` passes (UrlNormalizer unit tests)
3. All files exist in `src/main/java/dev/alexandria/crawl/` package (4 new files)
4. CrawlService wires PageDiscoveryService -> SitemapParser for sitemap-first discovery
5. CrawlService wires Crawl4AiClient for per-page crawling
6. CrawlService follows links only when no sitemap found (DiscoveryMethod.LINK_CRAWL)
7. UrlNormalizer handles fragments, trailing slashes, host casing, tracking params
</verification>

<success_criteria>
- System recursively crawls a documentation site from a root URL via Crawl4AI sidecar
- System checks for sitemap.xml and uses it for page discovery when available, falling back to recursive link crawling
- URLs are normalized to prevent duplicate crawling
- Crawl produces clean Markdown (boilerplate removed) for each discovered page
- Integration tests prove the complete crawl pipeline works against real containers
</success_criteria>

<output>
After completion, create `.planning/phases/03-web-crawling/03-02-SUMMARY.md`
</output>
