---
phase: 04.5-code-quality-consolidation
plan: 03
type: execute
wave: 2
depends_on: ["04.5-01"]
files_modified:
  - src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
  - src/test/java/dev/alexandria/crawl/PageDiscoveryServiceTest.java
  - src/test/java/dev/alexandria/crawl/SitemapParserTest.java
  - src/test/java/dev/alexandria/crawl/Crawl4AiClientTest.java
  - src/main/java/dev/alexandria/crawl/CrawlService.java
requirements: []
autonomous: true

must_haves:
  truths:
    - "CrawlService has unit tests covering happy path, error handling, and maxPages limit"
    - "PageDiscoveryService has unit tests covering sitemap-first strategy and link-crawl fallback"
    - "SitemapParser has unit tests covering sitemap parsing and error handling"
    - "Crawl4AiClient has unit tests covering successful crawl, error response, and RestClient exception"
    - "CrawlService.crawlSite() is refactored into smaller methods (under 30 lines each)"
  artifacts:
    - path: "src/test/java/dev/alexandria/crawl/CrawlServiceTest.java"
      provides: "Unit tests for CrawlService with mocked dependencies"
    - path: "src/test/java/dev/alexandria/crawl/PageDiscoveryServiceTest.java"
      provides: "Unit tests for PageDiscoveryService with mocked SitemapParser"
    - path: "src/test/java/dev/alexandria/crawl/SitemapParserTest.java"
      provides: "Unit tests for SitemapParser with mocked RestClient"
    - path: "src/test/java/dev/alexandria/crawl/Crawl4AiClientTest.java"
      provides: "Unit tests for Crawl4AiClient with mocked RestClient"
  key_links:
    - from: "CrawlServiceTest"
      to: "CrawlService"
      via: "mocks Crawl4AiClient + PageDiscoveryService"
      pattern: "@Mock.*Crawl4AiClient"
    - from: "Crawl4AiClientTest"
      to: "Crawl4AiClient"
      via: "mocks RestClient"
      pattern: "@Mock.*RestClient"
---

<objective>
Add unit tests for the entire crawl package (currently 0 unit tests except UrlNormalizer) and refactor CrawlService.crawlSite().

Purpose: The crawl package has the largest unit test gap. CrawlService, Crawl4AiClient, PageDiscoveryService, and SitemapParser have zero unit tests -- only integration tests exist. Add focused unit tests that verify behavior without requiring Docker containers.
Output: 4 new test classes, CrawlService.crawlSite refactored.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.5-code-quality-consolidation/04.5-RESEARCH.md
@src/main/java/dev/alexandria/crawl/CrawlService.java
@src/main/java/dev/alexandria/crawl/Crawl4AiClient.java
@src/main/java/dev/alexandria/crawl/PageDiscoveryService.java
@src/main/java/dev/alexandria/crawl/SitemapParser.java
@src/main/java/dev/alexandria/crawl/CrawlResult.java
@src/main/java/dev/alexandria/crawl/UrlNormalizer.java
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add unit tests for CrawlService and PageDiscoveryService</name>
  <files>
    src/test/java/dev/alexandria/crawl/CrawlServiceTest.java
    src/test/java/dev/alexandria/crawl/PageDiscoveryServiceTest.java
  </files>
  <action>
    **CrawlServiceTest** -- Mock Crawl4AiClient and PageDiscoveryService. Test:
    - Happy path: crawl site with single page, verify CrawlResult list returned
    - BFS link following: page returns internal links, verify they are crawled (LINK_CRAWL mode)
    - Sitemap mode: PageDiscoveryService returns SITEMAP result, verify links are crawled but not followed further
    - maxPages limit: verify crawling stops at configured limit
    - Error handling: Crawl4AiClient returns CrawlResult(success=false), verify it's skipped and crawling continues
    - URL deduplication: same URL discovered twice, verify it's only crawled once
    - URL normalization: verify UrlNormalizer is applied (fragments stripped, etc.)

    Use camelCase naming convention per user decision. Follow AAA structure. Constructor injection -- create CrawlService with mocked dependencies.

    **PageDiscoveryServiceTest** -- Mock SitemapParser. Test:
    - Sitemap available: SitemapParser returns URLs, verify SITEMAP result with URLs
    - No sitemap: SitemapParser throws/returns empty, verify LINK_CRAWL result
    - Mixed: verify sitemap-first strategy (tries sitemap before falling back)

    Use Mockito for mocking (@ExtendWith(MockitoExtension.class), @Mock, @InjectMocks pattern).
  </action>
  <verify>
    `./quality.sh test` passes all tests including new CrawlServiceTest and PageDiscoveryServiceTest.
  </verify>
  <done>CrawlServiceTest covers happy path, BFS, sitemap mode, maxPages, error handling, dedup. PageDiscoveryServiceTest covers sitemap-first strategy and fallback. All tests use camelCase naming.</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for Crawl4AiClient and SitemapParser, refactor CrawlService.crawlSite()</name>
  <files>
    src/test/java/dev/alexandria/crawl/Crawl4AiClientTest.java
    src/test/java/dev/alexandria/crawl/SitemapParserTest.java
    src/main/java/dev/alexandria/crawl/CrawlService.java
  </files>
  <action>
    **Crawl4AiClientTest** -- Mock RestClient (or use RestClient.Builder mock chain). Test:
    - Successful crawl: valid response with markdown content, verify CrawlResult mapping
    - Error response: Crawl4AI returns success=false in response, verify CrawlResult(success=false)
    - RestClientException: HTTP error thrown, verify CrawlResult(success=false, errorMessage) returned (not propagated)
    - Null/empty markdown handling: verify graceful behavior

    RestClient mocking can be complex with builder chains. Read Crawl4AiClient.java to understand how RestClient is used, then mock at the appropriate level. If RestClient mocking is too brittle, consider using MockRestServiceServer or a simple stub approach.

    **SitemapParserTest** -- Mock RestClient.Builder for HTTP calls. Test:
    - Valid sitemap.xml: returns list of URLs parsed from sitemap
    - Sitemap index: returns URLs from nested sitemaps
    - No sitemap found: all attempts fail, returns empty list
    - Malformed XML: graceful error handling

    SitemapParser uses crawler-commons internally. Mock the HTTP layer, not crawler-commons itself.

    **Refactor CrawlService.crawlSite() (~50 lines):**
    The BFS crawl loop does multiple distinct things per iteration: dequeue URL, normalize, skip-if-visited, crawl page, handle success/failure, discover links. Extract:
    - `processNextUrl(...)` or similar -- handles a single URL from the queue
    - Or extract the success-path handling (link discovery + result collection) into a named method

    Keep the refactoring pragmatic per user decision. The goal is readability, not hitting an arbitrary line count. After refactoring, run:
    ```bash
    ./quality.sh test
    ```
  </action>
  <verify>
    `./quality.sh test` passes all tests including new Crawl4AiClientTest and SitemapParserTest. CrawlService.crawlSite() main body is under 30 lines after refactoring.
  </verify>
  <done>Crawl4AiClientTest covers success, error, exception, null cases. SitemapParserTest covers valid/index/missing/malformed cases. CrawlService.crawlSite() refactored for readability. All tests use camelCase naming.</done>
</task>

</tasks>

<verification>
- `./quality.sh test` passes all unit tests (old + new)
- 4 new test files exist: CrawlServiceTest, PageDiscoveryServiceTest, SitemapParserTest, Crawl4AiClientTest
- All new tests use camelCase naming
- CrawlService.crawlSite() refactored (main body under 30 lines)
- PIT mutation score for crawl package improved (run `./quality.sh mutation --package dev.alexandria.crawl`)
</verification>

<success_criteria>
Crawl package goes from 1 unit test class (UrlNormalizerTest) to 5. CrawlService.crawlSite() is more readable. All quality gates pass.
</success_criteria>

<output>
After completion, create `.planning/phases/04.5-code-quality-consolidation/04.5-03-SUMMARY.md`
</output>
